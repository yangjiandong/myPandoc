

%ğŸ% \chapter{Case study: data structure selection  |  æ¡ˆä¾‹ç ”ç©¶ï¼šæ•°æ®ç»“æ„é€‰æ‹©}
\chapter{æ¡ˆä¾‹ç ”ç©¶ï¼šæ•°æ®ç»“æ„é€‰æ‹©}

%ğŸ% At this point you have learned about Python's core data structures,
%ğŸ% and you have seen some of the algorithms that use them.
%ğŸ% If you would like to know more about algorithms, this might be a good
%ğŸ% time to read Chapter~\ref{algorithms}.
%ğŸ% But you don't have to read it before you go on; you can read
%ğŸ% it whenever you are interested.

ç›®å‰ä¸ºæ­¢ï¼Œ ä½ å·²ç»å­¦å®Œäº† Python çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œ åŒæ—¶ä½ ä¹Ÿæ¥è§¦äº†åˆ©ç”¨åˆ°è¿™äº›æ•°æ®ç»“æ„çš„ä¸€äº›ç®—æ³•ã€‚
å¦‚æœä½ å¸Œæœ›å­¦ä¹ æ›´å¤šç®—æ³•çŸ¥è¯†ï¼Œ é‚£ä¹ˆç°åœ¨æ˜¯é˜…è¯» é™„å½•~\ref{algorithms} çš„å¥½æ—¶æœºã€‚
ä½†æ˜¯ä¸å¿…æ€¥ç€é©¬ä¸Šè¯»ï¼Œ ä»€ä¹ˆæ—¶å€™æ„Ÿå…´è¶£äº†å†å»è¯»å³å¯ã€‚

%ğŸ% This chapter presents a case study with exercises that let
%ğŸ% you think about choosing data structures and practice using them.

æœ¬ç« æ˜¯ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œ åŒæ—¶ç»™å‡ºäº†ä¸€äº›ä¹ é¢˜ï¼Œ ç›®çš„æ˜¯å¯å‘ä½ æ€è€ƒå¦‚ä½•é€‰æ‹©æ•°æ®ç»“æ„ï¼Œ å¹¶ç»ƒä¹ æ•°æ®ç»“æ„ä½¿ç”¨ã€‚

%ğŸ% \section{Word frequency analysis  |  è¯é¢‘åˆ†æ}
\section{è¯é¢‘åˆ†æ}
\label{analysis}

%ğŸ% As usual, you should at least attempt the exercises
%ğŸ% before you read my solutions.

å’Œä¹‹å‰ä¸€æ ·ï¼Œ åœ¨æŸ¥çœ‹ç­”æ¡ˆä¹‹å‰ï¼Œ ä½ å¯ä»¥è¯•ç€è§£ç­”ä¸€ä¸‹è¿™äº›ä¹ é¢˜ã€‚

\begin{exercise}

%ğŸ% Write a program that reads a file, breaks each line into
%ğŸ% words, strips whitespace and punctuation from the words, and
%ğŸ% converts them to lowercase.

ç¼–å†™ä¸€ä¸ªç¨‹åºï¼Œ è¯»å–ä¸€ä¸ªæ–‡ä»¶ï¼Œ å°†æ¯ä¸€è¡Œè½¬æ¢æˆå•è¯åˆ—è¡¨ï¼Œ åˆ æ‰å•è¯ä¸­çš„ç©ºæ ¼å’Œæ ‡ç‚¹ï¼Œ ç„¶åå°†å®ƒä»¬è½¬æ¢ä¸ºå°å†™å­—æ¯ã€‚

\index{string module}
\index{module!string}

%ğŸ% Hint: The {\tt string} module provides a string named {\tt whitespace},
%ğŸ% which contains space, tab, newline, etc., and {\tt
%ğŸ%   punctuation} which contains the punctuation characters.  Let's see
%ğŸ% if we can make Python swear:

æç¤ºï¼š {\em \li{string}} æ¨¡å—æä¾›äº†åä¸º {\em \li{whitespace}} çš„å­—ç¬¦ä¸²ï¼Œ
å…¶åŒ…æ‹¬ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ–°è¡Œç­‰ç­‰ï¼Œ ä»¥åŠåä¸º {\em \li{punctuation}} çš„å­—ç¬¦ä¸²ï¼Œ
å…¶åŒ…æ‹¬æ ‡ç‚¹å­—ç¬¦ã€‚   è¯•è¯•èƒ½å¦è®© {\em Python} è¯´è„è¯ï¼š

\begin{em}
\begin{lstlisting}
>>> import string
>>> string.punctuation
'!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~'
\end{lstlisting}
\end{em}

%ğŸ% %
%ğŸ% Also, you might consider using the string methods {\tt strip},
%ğŸ% {\tt replace} and {\tt translate}.

åŒæ—¶ï¼Œ ä½ å¯ä»¥è€ƒè™‘ä½¿ç”¨å­—ç¬¦ä¸²æ–¹æ³• {\em \li{strip}} ã€ {\em \li{replace}} å’Œ {\em \li{translate}}ã€‚

\index{strip method}
\index{method!strip}
\index{replace method}
\index{method!replace}
\index{translate method}
\index{method!translate}
\end{exercise}

\begin{exercise}
\index{Project Gutenberg}

%ğŸ% Go to Project Gutenberg (\url{http://gutenberg.org}) and download
%ğŸ% your favorite out-of-copyright book in plain text format.

å‰å¾€\href{http://gutenberg.org}{å¤ç™»å ¡é¡¹ç›®\footnote{Project Gutenberg, \url{http://gutenberg.org}.}}ï¼Œ ä»¥çº¯æ–‡æœ¬æ ¼å¼ä¸‹è½½ä½ å–œæ¬¢çš„å·²æ— ç‰ˆæƒä¿æŠ¤çš„å›¾ä¹¦ã€‚

\index{plain text}
\index{text!plain}

%ğŸ% Modify your program from the previous exercise to read the book
%ğŸ% you downloaded, skip over the header information at the beginning
%ğŸ% of the file, and process the rest of the words as before.
%ğŸ% Then modify the program to count the total number of words in
%ğŸ% the book, and the number of times each word is used.

ä¿®æ”¹å‰é¢ä¹ é¢˜çš„ç¨‹åºï¼Œ è¯»å–ä½ ä¸‹è½½çš„ä¹¦ï¼Œ
è·³è¿‡æ–‡ä»¶å¼€å§‹çš„å¤´éƒ¨ä¿¡æ¯ï¼Œ åƒä¹‹å‰é‚£æ ·å¤„ç†å…¶ä½™çš„å•è¯ã€‚
ç„¶åä¿®æ”¹ç¨‹åºï¼Œ è®¡ç®—ä¹¦ä¸­å•è¯çš„æ€»æ•°ï¼Œ ä»¥åŠæ¯ä¸ªå•è¯ä½¿ç”¨çš„æ¬¡æ•°ã€‚

\index{word frequency}
\index{frequency!word}

%ğŸ% Print the number of different words used in the book.  Compare
%ğŸ% different books by different authors, written in different eras.
%ğŸ% Which author uses the most extensive vocabulary?

æ‰“å°è¯¥ä¹¦ä½¿ç”¨å•è¯çš„æ€»æ•°ã€‚   æ¯”è¾ƒä¸åŒå¹´ä»£ã€ä¸åŒä½œè€…å†™çš„ä¹¦ã€‚
å“ªä¸ªä½œè€…ä½¿ç”¨çš„è¯æ±‡é‡æœ€å¤§ï¼Ÿ

\end{exercise}

\begin{exercise}
%ğŸ% Modify the program from the previous exercise to print the
%ğŸ% 20 most frequently used words in the book.

ä¿®æ”¹ä¸Šä¸€ä¸ªä¹ é¢˜ä¸­çš„ç¨‹åºï¼Œ æ‰“å°ä¹¦ä¸­æœ€å¸¸ä½¿ç”¨çš„ {\em 20} ä¸ªå•è¯ã€‚
\end{exercise}

\begin{exercise}
%ğŸ% Modify the previous program to read a word list (see
%ğŸ% Section~\ref{wordlist}) and then print all the words in the book that
%ğŸ% are not in the word list.  How many of them are typos?  How many of
%ğŸ% them are common words that {\em should} be in the word list, and how
%ğŸ% many of them are really obscure?

ä¿®æ”¹ä¸Šä¸€ä¸ªä¹ é¢˜ä¸­çš„ç¨‹åºï¼Œ è¯»å–ä¸€ä¸ªå•è¯åˆ—è¡¨(è§ {\em \ref{wordlist}}~èŠ‚)ï¼Œ
ç„¶åæ‰“å°ä¹¦ä¸­æ‰€æœ‰æ²¡æœ‰å‡ºç°åœ¨è¯¥å•è¯è¡¨ä¸­çš„å•è¯ã€‚
å®ƒä»¬ä¸­æœ‰å¤šå°‘æ˜¯æ‹¼å†™é”™è¯¯çš„ï¼Ÿæœ‰å¤šå°‘æ˜¯è¯è¡¨ä¸­ {\bf åº”è¯¥} åŒ…æ‹¬çš„å¸¸ç”¨è¯ï¼Ÿæœ‰å¤šå°‘æ˜¯ç”Ÿåƒ»è¯ï¼Ÿ
\end{exercise}

%ğŸ% \section{Random numbers  |  éšæœºæ•°}
\section{éšæœºæ•°}
\index{random number}
\index{number, random}
\index{deterministic}
\index{pseudorandom}

%ğŸ% Given the same inputs, most computer programs generate the same
%ğŸ% outputs every time, so they are said to be {\bf deterministic}.
%ğŸ% Determinism is usually a good thing, since we expect the same
%ğŸ% calculation to yield the same result.  For some applications, though,
%ğŸ% we want the computer to be unpredictable.  Games are an obvious
%ğŸ% example, but there are more.

ç»™å®šç›¸åŒçš„è¾“å…¥ï¼Œ å¤§å¤šæ•°è®¡ç®—æœºç¨‹åºæ¯æ¬¡éƒ½ä¼šç”Ÿæˆç›¸åŒçš„è¾“å‡ºï¼Œ
å®ƒä»¬å› æ­¤è¢«ç§°ä½œ {\bf ç¡®å®šæ€§çš„\footnote{deterministic}}  ã€‚
ç¡®å®šæ€§é€šå¸¸æ˜¯ä¸ªå¥½ä¸œè¥¿ï¼Œ å› ä¸ºæˆ‘ä»¬æœŸæœ›ç›¸åŒçš„è®¡ç®—äº§ç”Ÿç›¸åŒçš„ç»“æœã€‚
ç„¶è€Œï¼Œ å¯¹äºæœ‰äº›åº”ç”¨ï¼Œ æˆ‘ä»¬å¸Œæœ›è®¡ç®—æœºä¸å¯é¢„çŸ¥ã€‚
æ¸¸æˆæ˜¯ä¸€ä¸ªæ˜æ˜¾çš„ä¾‹å­ï¼Œ ä½†æ˜¯ä¸é™äºæ­¤ã€‚

%ğŸ% Making a program truly nondeterministic turns out to be difficult,
%ğŸ% but there are ways to make it at least seem nondeterministic.  One of
%ğŸ% them is to use algorithms that generate {\bf pseudorandom} numbers.
%ğŸ% Pseudorandom numbers are not truly random because they are generated
%ğŸ% by a deterministic computation, but just by looking at the numbers it
%ğŸ% is all but impossible to distinguish them from random.

è®©ç¨‹åºå…·å¤‡çœŸæ­£æ„ä¹‰ä¸Šçš„éç¡®å®šæ€§å¹¶ä¸å®¹æ˜“ï¼Œ ä½†æ˜¯æœ‰åŠæ³•ä½¿å®ƒè‡³å°‘çœ‹èµ·æ¥æ˜¯ä¸ç¡®å®šçš„ã€‚
å…¶ä¸­ä¹‹ä¸€æ˜¯ä½¿ç”¨ç”Ÿæˆ {\em ä¼ªéšæœºæ•°} (pseudorandom) çš„ç®—æ³•ã€‚
ä¼ªéšæœºæ•°ä¸æ˜¯çœŸæ­£çš„éšæœºæ•°ï¼Œ å› ä¸ºå®ƒä»¬ç”±ä¸€ä¸ªç¡®å®šæ€§çš„è®¡ç®—ç”Ÿæˆï¼Œ
ä½†æ˜¯ä»…çœ‹å…¶ç”Ÿæˆçš„æ•°å­—ï¼Œ ä¸å¯èƒ½å°†å®ƒä»¬å’Œéšæœºç”Ÿæˆçš„ç›¸åŒºåˆ†å¼€ã€‚

\index{random module}
\index{module!random}

%ğŸ% The {\tt random} module provides functions that generate
%ğŸ% pseudorandom numbers (which I will simply call ``random'' from
%ğŸ% here on).

\li{random} æ¨¡å—æä¾›äº†ç”Ÿæˆä¼ªéšæœºæ•°(ä¸‹æ–‡ä¸­ç®€ç§°ä¸º``éšæœºæ•°'')çš„å‡½æ•°ã€‚

\index{random function}
\index{function!random}

%ğŸ% The function {\tt random} returns a random float
%ğŸ% between 0.0 and 1.0 (including 0.0 but not 1.0).  Each time you
%ğŸ% call {\tt random}, you get the next number in a long series.  To see a
%ğŸ% sample, run this loop:

å‡½æ•° \li{random} è¿”å›ä¸€ä¸ª 0.0 åˆ° 1.0 ä¹‹é—´çš„éšæœºæµ®ç‚¹æ•°(åŒ…æ‹¬ 0.0 ï¼Œ ä½†æ˜¯ä¸åŒ…æ‹¬ 1.0 )ã€‚    æ¯æ¬¡è°ƒç”¨ \li{random} ï¼Œ ä½ è·å¾—ä¸€ä¸ªé•¿åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ•°ã€‚
ä¸¾ä¸ªä¾‹å­ï¼Œ è¿è¡Œæ­¤å¾ªç¯ï¼š

\begin{lstlisting}
import random
for i in range(10):
    x = random.random()
    print(x)
\end{lstlisting}

%ğŸ% %
%ğŸ% The function {\tt randint} takes parameters {\tt low} and
%ğŸ% {\tt high} and returns an integer between {\tt low} and
%ğŸ% {\tt high} (including both).

å‡½æ•° \li{randint} æ¥å—å‚æ•° \li{low} å’Œ \li{high} ï¼Œ
è¿”å›ä¸€ä¸ª \li{low} å’Œ \li{high} ä¹‹é—´çš„æ•´æ•°(ä¸¤ä¸ªéƒ½åŒ…æ‹¬)ã€‚

\index{randint function}
\index{function!randint}

\begin{lstlisting}
>>> random.randint(5, 10)
5
>>> random.randint(5, 10)
9
\end{lstlisting}

%ğŸ% %
%ğŸ% To choose an element from a sequence at random, you can use
%ğŸ% {\tt choice}:

ä½ å¯ä»¥ä½¿ç”¨ \li{choice} ï¼Œ ä»ä¸€ä¸ªåºåˆ—ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå…ƒç´ ï¼š

\index{choice function}
\index{function!choice}

\begin{lstlisting}
>>> t = [1, 2, 3]
>>> random.choice(t)
2
>>> random.choice(t)
3
\end{lstlisting}

%ğŸ% %
%ğŸ% The {\tt random} module also provides functions to generate
%ğŸ% random values from continuous distributions including
%ğŸ% Gaussian, exponential, gamma, and a few more.

\li{random} æ¨¡å—æä¾›çš„å‡½æ•°ï¼Œ è¿˜å¯ä»¥ç”Ÿæˆç¬¦åˆé«˜æ–¯ã€æŒ‡æ•°ã€ä¼½é©¬ç­‰è¿ç»­åˆ†å¸ƒçš„éšæœºå€¼ã€‚

\begin{exercise}
\index{histogram!random choice}

%ğŸ% Write a function named \verb"choose_from_hist" that takes
%ğŸ% a histogram as defined in Section~\ref{histogram} and returns a
%ğŸ% random value from the histogram, chosen with probability
%ğŸ% in proportion to frequency.  For example, for this histogram:

ç¼–å†™ä¸€ä¸ªåä¸º {\em \li{choose_from_hist}} çš„å‡½æ•°ï¼Œ  å…¶æ¥å—ä¸€ä¸ªå¦‚ {\em \ref{histogram}} ä¸€èŠ‚ä¸­å®šä¹‰çš„ {\em \li{histogram}} å¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œ  å¹¶ä»è¯¥å¯¹è±¡ä¸­è¿”å›ä¸€ä¸ªéšæœºå€¼ï¼Œ  å…¶é€‰æ‹©æ¦‚ç‡å’Œå€¼å‡ºç°çš„é¢‘ç‡æˆæ­£æ¯”ã€‚   ä¾‹å¦‚ï¼š

\begin{em}
\begin{lstlisting}
>>> t = ['a', 'a', 'b']
>>> hist = histogram(t)
>>> hist
{'a': 2, 'b': 1}
\end{lstlisting}
\end{em}

%ğŸ% %
%ğŸ% your function should return \verb"'a'" with probability $2/3$ and \verb"'b'"
%ğŸ% with probability $1/3$.

ä½ çš„å‡½æ•°è¿”å› {\em \li{'a'}} çš„æ¦‚ç‡åº”è¯¥æ˜¯ {\em $2/3$} ï¼Œ è¿”å› {\em \li{'b'}} çš„æ¦‚ç‡åº”è¯¥æ˜¯ {\em $1/3$} ã€‚

\end{exercise}

%ğŸ% \section{Word histogram  |  å•è¯ç›´æ–¹å›¾}
\section{å•è¯ç›´æ–¹å›¾}

%ğŸ% You should attempt the previous exercises before you go on.
%ğŸ% You can download my solution from
%ğŸ%  \url{http://thinkpython2.com/code/analyze_book1.py}.  You will
%ğŸ% also need \url{http://thinkpython2.com/code/emma.txt}.
%ğŸ% Here is a program that reads a file and builds a histogram of the
%ğŸ% words in the file:

åœ¨ç»§ç»­ä¸‹é¢çš„ä¹ é¢˜ä¹‹å‰ï¼Œ ä½ åº”è¯¥å°è¯•å®Œæˆå‰é¢çš„ç»ƒä¹ ã€‚
ä½ å¯ä»¥ä¸‹è½½ \href{http://thinkpython2.com/code/analyze_book1.py}{æˆ‘çš„ç­”æ¡ˆ}ã€‚
ä½ è¿˜éœ€è¦ä¸‹è½½\href{http://thinkpython2.com/code/emma.txt}{è¿™ä¸ªæ–‡ä»¶} ã€‚
ä¸‹é¢è¿™ä¸ªç¨‹åºå°†è¯»å–ä¸€ä¸ªæ–‡ä»¶ï¼Œ å¹¶å»ºç«‹æ–‡ä»¶ä¸­å•è¯çš„ç›´æ–¹å›¾ï¼š

\index{histogram!word frequencies}

\begin{lstlisting}
import string
def process_file(filename):
    hist = dict()
    fp = open(filename)
    for line in fp:
        process_line(line, hist)
    return hist
def process_line(line, hist):
    line = line.replace('-', ' ')
    for word in line.split():
        word = word.strip(string.punctuation + string.whitespace)
        word = word.lower()
        hist[word] = hist.get(word, 0) + 1
hist = process_file('emma.txt')
\end{lstlisting}

%ğŸ% %
%ğŸ% This program reads {\tt emma.txt}, which contains the text of {\em
%ğŸ%   Emma} by Jane Austen.

è¯¥ç¨‹åºè¯»å– \li{emma.txt} ï¼Œ å…¶åŒ…æ‹¬ Jane Austen çš„å°è¯´ ã€Š{\em Emma}ã€‹ æ–‡æœ¬ã€‚

\index{Austin, Jane}

%ğŸ% \verb"process_file" loops through the lines of the file,
%ğŸ% passing them one at a time to \verb"process_line".  The histogram
%ğŸ% {\tt hist} is being used as an accumulator.

\li{process_file} å¾ªç¯è¯»å–æ–‡ä»¶çš„æ¯è¡Œï¼Œ ä¾æ¬¡æŠŠå®ƒä»¬ä¼ é€’ç»™ \li{process_line} ã€‚
ç›´æ–¹å›¾ \li{hist} è¢«ç”¨ä½œä¸€ä¸ªç´¯åŠ å™¨ã€‚

\index{accumulator!histogram}
\index{traversal}

%ğŸ% \verb"process_line" uses the string method {\tt replace} to replace
%ğŸ% hyphens with spaces before using {\tt split} to break the line into a
%ğŸ% list of strings.  It traverses the list of words and uses {\tt strip}
%ğŸ% and {\tt lower} to remove punctuation and convert to lower case.  (It
%ğŸ% is a shorthand to say that strings are ``converted''; remember that
%ğŸ% strings are immutable, so methods like {\tt strip} and {\tt lower}
%ğŸ% return new strings.)

\li{process_line} ä½¿ç”¨å­—ç¬¦ä¸²çš„ \li{replace} æ–¹æ³•å°†è¿å­—ç¬¦æ›¿æ¢æˆç©ºæ ¼ã€‚
å®ƒä¼šéå†å•è¯åˆ—è¡¨ï¼Œ å¹¶ä½¿ç”¨ \li{strip} å’Œ \li{lower} æ¥åˆ é™¤æ ‡ç‚¹ä»¥åŠå°†å•è¯è½¬æ¢ä¸ºå°å†™ã€‚
(``è½¬æ¢'' åªæ˜¯ä¸€ç§ç®€ç•¥çš„è¯´æ³•ï¼›è®°ä½ï¼Œ å­—ç¬¦ä¸²æ˜¯ä¸å¯å˜çš„ï¼Œ æ‰€ä»¥ç±»ä¼¼ \li{strip} å’Œ \li{lower} è¿™æ ·çš„æ–¹æ³•å…¶å®è¿”å›çš„æ˜¯æ–°å­—ç¬¦ä¸²ã€‚  )

%ğŸ% Finally, \verb"process_line" updates the histogram by creating a new
%ğŸ% item or incrementing an existing one.

æœ€åï¼Œ \li{process_line} é€šè¿‡ç”Ÿæˆä¸€ä¸ªæ–°çš„é¡¹æˆ–è€…é€’å¢ä¸€ä¸ªå·²æœ‰çš„é¡¹æ¥æ›´æ–°ç›´æ–¹

\index{update!histogram}

%ğŸ% To count the total number of words in the file, we can add up
%ğŸ% the frequencies in the histogram:

æˆ‘ä»¬å¯ä»¥é€šè¿‡ç´¯åŠ ç›´æ–¹å›¾ä¸­çš„é¢‘ç‡ï¼Œ æ¥ç»Ÿè®¡æ–‡ä»¶ä¸­çš„å•è¯æ€»æ•°ï¼š

\begin{lstlisting}
def total_words(hist):
    return sum(hist.values())
\end{lstlisting}

%
%ğŸ% The number of different words is just the number of items in
%ğŸ% the dictionary:

ä¸åŒå•è¯çš„æ•°é‡æ°å¥½æ˜¯è¯å…¸ä¸­é¡¹çš„æ•°ç›®ï¼š

\begin{lstlisting}
def different_words(hist):
    return len(hist)
\end{lstlisting}

%ğŸ% %
%ğŸ% Here is some code to print the results:

ä»¥ä¸‹æ‰“å°ç»“æœçš„ä»£ç ï¼š

\begin{lstlisting}
print('Total number of words:', total_words(hist))
print('Number of different words:', different_words(hist))
\end{lstlisting}

%ğŸ% %
%ğŸ% And the results:

å…¶ç»“æœæ˜¯ï¼š

\begin{lstlisting}
Total number of words: 161080
Number of different words: 7214
\end{lstlisting}

%
%ğŸ% \section{Most common words  |  æœ€å¸¸ç”¨å•è¯}
\section{æœ€å¸¸ç”¨å•è¯}

%ğŸ% To find the most common words, we can make a list of tuples,
%ğŸ% where each tuple contains a word and its frequency,
%ğŸ% and sort it.
%ğŸ% The following function takes a histogram and returns a list of
%ğŸ% word-frequency tuples:

ä¸ºäº†æ‰¾åˆ°æœ€å¸¸ç”¨çš„å•è¯ï¼Œ æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å…ƒç»„åˆ—è¡¨ï¼Œ å…¶ä¸­æ¯ä¸ªå…ƒç»„åŒ…å«å•è¯å’Œå®ƒçš„é¢‘ç‡ï¼Œ ç„¶åæ’åºè¿™ä¸ªåˆ—è¡¨ã€‚

ä¸‹é¢çš„å‡½æ•°æ¥å—ä¸€ä¸ªç›´æ–¹å›¾å¹¶ä¸”è¿”å›ä¸€ä¸ª
å•è¯-é¢‘ç‡çš„å…ƒç»„åˆ—è¡¨ï¼š

\begin{lstlisting}
def most_common(hist):
    t = []
    for key, value in hist.items():
        t.append((value, key))
    t.sort(reverse=True)
    return t
\end{lstlisting}

%ğŸ% In each tuple, the frequency appears first, so the resulting list is
%ğŸ% sorted by frequency.  Here is a loop that prints the ten most common
%ğŸ% words:

æ¯ä¸€ä¸ªå…ƒç»„ä¸­ï¼Œ é¢‘ç‡åœ¨å‰ï¼Œ æ‰€ä»¥è¿™ä¸ªåˆ—è¡¨æ˜¯æŒ‰ç…§é¢‘ç‡æ’åºã€‚
ä¸‹é¢æ˜¯è¾“å‡ºæœ€å¸¸ç”¨çš„åä¸ªå•è¯çš„å¾ªç¯ï¼š

\begin{lstlisting}
t = most_common(hist)
print('The most common words are:')
for freq, word in t[:10]:
    print(word, freq, sep='\t')
\end{lstlisting}

%ğŸ% %
%ğŸ% I use the keyword argument {\tt sep} to tell {\tt print} to use a tab
%ğŸ% character as a ``separator'', rather than a space, so the second
%ğŸ% column is lined up.  Here are the results from {\em Emma}:

è¿™é‡Œæˆ‘é€šè¿‡å…³é”®è¯å‚æ•° \li{sep}ï¼Œ  è®© \li{print} ä½¿ç”¨ä¸€ä¸ªåˆ¶è¡¨ç¬¦(Tab)è€Œä¸æ˜¯ç©ºæ ¼é”®ä½œä¸ºåˆ†éš”ç¬¦ï¼Œ  æ‰€ä»¥ç¬¬äºŒè¡Œå°†å¯¹é½ã€‚    ä¸‹é¢æ˜¯å¯¹ ã€Š{\em Emma}ã€‹ çš„åˆ†æç»“æœï¼š

\begin{lstlisting}
The most common words are:
to      5242
the     5205
and     4897
of      4295
i       3191
a       3130
it      2529
her     2483
was     2400
she     2364
\end{lstlisting}

%ğŸ% %
%ğŸ% This code can be simplified using the {\tt key} parameter of
%ğŸ% the {\tt sort} function.  If you are curious, you can read about it
%ğŸ% at \url{https://wiki.python.org/moin/HowTo/Sorting}.

å½“ç„¶ï¼Œ è¿™æ®µä»£ç ä¹Ÿå¯ä»¥é€šè¿‡ \li{sort} å‡½æ•°çš„å‚æ•° \li{key} è¿›è¡Œç®€åŒ–ã€‚
å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œ å¯ä»¥é˜…è¯»\href{https://wiki.python.org/moin/HowTo/Sorting}{è¿™ç¯‡æ–‡ç« } ã€‚

%ğŸ% \section{Optional parameters  |  å¯é€‰å½¢å‚}
\section{å¯é€‰å½¢å‚}

\index{optional parameter}
\index{parameter!optional}

%ğŸ% We have seen built-in functions and methods that take optional
%ğŸ% arguments.  It is possible to write programmer-defined functions
%ğŸ% with optional arguments, too.  For example, here is a function that
%ğŸ% prints the most common words in a histogram

æˆ‘ä»¬å·²ç»è§è¿‡æ¥å—å¯å˜æ•°é‡å®å‚çš„å‡½æ•°å’Œæ–¹æ³•äº†ã€‚
ç¨‹åºå‘˜ä¹Ÿå¯ä»¥è‡ªå·±å®šä¹‰å…·æœ‰å¯é€‰å®å‚çš„å‡½æ•°ã€‚
ä¾‹å¦‚ï¼Œ ä¸‹é¢å°±æ˜¯ä¸€ä¸ªæ‰“å°ç›´æ–¹å›¾ä¸­æœ€å¸¸è§å•è¯çš„å‡½æ•°ã€‚

\index{programmer-defined function}
\index{function!programmer defined}

\begin{lstlisting}
def print_most_common(hist, num=10):
    t = most_common(hist)
    print('The most common words are:')
    for freq, word in t[:num]:
        print(word, freq, sep='\t')
\end{lstlisting}

%ğŸ% The first parameter is required; the second is optional.
%ğŸ% The {\bf default value} of {\tt num} is 10.

ç¬¬ä¸€ä¸ªå½¢å‚æ˜¯å¿…é¡»çš„ï¼›ç¬¬äºŒä¸ªæ˜¯å¯é€‰çš„ã€‚   \li{num} çš„\ **é»˜è®¤å€¼(default
value)**\ æ˜¯10ã€‚

\index{default value}
\index{value!default}

%ğŸ% If you only provide one argument:

å¦‚æœä½ åªæä¾›äº†ä¸€ä¸ªå®å‚:

\begin{lstlisting}
print_most_common(hist)
\end{lstlisting}
{\tt num} gets the default value.  If you provide two arguments:
\begin{lstlisting}
print_most_common(hist, 20)
\end{lstlisting}
{\tt num} gets the value of the argument instead.  In other
words, the optional argument {\bf overrides} the default value.

\index{override}

%ğŸ% If a function has both required and optional parameters, all
%ğŸ% the required parameters have to come first, followed by the
%ğŸ% optional ones.

å¦‚æœä¸€ä¸ªå‡½æ•°åŒæ—¶æœ‰å¿…é€‰å’Œå¯é€‰ä¸¤ç±»å½¢å‚ï¼Œ åˆ™æ‰€æœ‰çš„å¿…é€‰å½¢å‚å¿…é¡»é¦–å…ˆå‡ºç°ï¼Œ å¯é€‰å½¢å‚ç´§éšå…¶åã€‚

%ğŸ% \section{Dictionary subtraction  |  å­—å…¸å·®é›†}
\section{å­—å…¸å·®é›†}

\label{dictsub}
\index{dictionary!subtraction}
\index{subtraction!dictionary}

%ğŸ% Finding the words from the book that are not in the word list
%ğŸ% from {\tt words.txt} is a problem you might recognize as set
%ğŸ% subtraction; that is, we want to find all the words from one
%ğŸ% set (the words in the book) that are not in the other (the
%ğŸ% words in the list).

ä»ä¹¦ä¸­æ‰¾åˆ°æ‰€æœ‰æ²¡å‡ºç°åœ¨è¯è¡¨ \li{words.txt} ä¸­çš„å•è¯ï¼Œ å¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªå·®é›†é—®é¢˜ï¼›
ä¹Ÿå°±æ˜¯ï¼Œ æˆ‘ä»¬åº”è¯¥ä»ä¸€ä¸ªé›†åˆä¸­(ä¹¦ä¸­çš„å•è¯)æ‰¾åˆ°æ‰€æœ‰æ²¡å‡ºç°åœ¨å¦ä¸€ä¸ªé›†åˆä¸­
(åˆ—è¡¨ä¸­çš„å•è¯)çš„å•è¯ã€‚

%ğŸ% {\tt subtract} takes dictionaries {\tt d1} and {\tt d2} and returns a
%ğŸ% new dictionary that contains all the keys from {\tt d1} that are not
%ğŸ% in {\tt d2}.  Since we don't really care about the values, we
%ğŸ% set them all to None.

\li{subtract} æ¥å—è¯å…¸ \li{d1} å’Œ \li{d2} ï¼Œ å¹¶è¿”å›ä¸€ä¸ªæ–°çš„è¯å…¸ï¼Œ
å…¶åŒ…æ‹¬ \li{d1} ä¸­çš„æ‰€æœ‰æ²¡å‡ºç°åœ¨ \li{d2} ä¸­çš„é”®ã€‚
ç”±äºå¹¶ä¸çœŸæ­£å…³å¿ƒå€¼æ˜¯ä»€ä¹ˆï¼Œ æˆ‘ä»¬å°†å®ƒä»¬éƒ½è®¾ä¸º \li{None}ã€‚

\begin{lstlisting}
def subtract(d1, d2):
    res = dict()
    for key in d1:
        if key not in d2:
            res[key] = None
    return res
\end{lstlisting}

%ğŸ% %
%ğŸ% To find the words in the book that are not in {\tt words.txt},
%ğŸ% we can use \verb"process_file" to build a histogram for
%ğŸ% {\tt words.txt}, and then subtract:

ä¸ºäº†æ‰¾åˆ°ä¹¦ä¸­æ²¡æœ‰å‡ºç°åœ¨ \li{words.txt} ä¸­çš„å•è¯ï¼Œ
æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ \li{process_file} æ¥ä¸º \li{words.txt} æ„å»ºä¸€ä¸ªç›´æ–¹å›¾ï¼Œ
ç„¶åä½¿ç”¨ \li{subtract} ï¼š

\begin{lstlisting}
words = process_file('words.txt')
diff = subtract(hist, words)
print("Words in the book that aren't in the word list:")
for word in diff.keys():
    print(word, end=' ')
\end{lstlisting}

%ğŸ% %
%ğŸ% Here are some of the results from {\em Emma}:

è¿™æ˜¯é’ˆå¯¹å°è¯´ ã€Š{\em Emma}ã€‹ çš„éƒ¨åˆ†è¿è¡Œç»“æœï¼š

\begin{lstlisting}
Words in the book that aren't in the word list:
rencontre jane's blanche woodhouses disingenuousness
friend's venice apartment ...
\end{lstlisting}

%ğŸ% %
%ğŸ% Some of these words are names and possessives.  Others, like
%ğŸ% ``rencontre'', are no longer in common use.  But a few are common
%ğŸ% words that should really be in the list!

è¿™äº›å•è¯ä¸­ï¼Œ ä¸€äº›æ˜¯åå­—å’Œåè¯æ‰€æœ‰æ ¼ã€‚  å¦‚â€œrencontreâ€è¿™æ ·çš„å…¶ä»–å•è¯å·²ç»ä¸å¸¸ä½¿ç”¨äº†ã€‚
ä½†æ˜¯æœ‰ä¸€äº›çœŸçš„åº”è¯¥åŒ…æ‹¬åœ¨åˆ—è¡¨ä¸­ï¼

\begin{exercise}
\index{set}
\index{type!set}
%ğŸ% Python provides a data structure called {\tt set} that provides many
%ğŸ% common set operations.  You can read about them in Section~\ref{sets},
%ğŸ% or read the documentation at
%ğŸ% \url{http://docs.python.org/3/library/stdtypes.html#types-set}.
%ğŸ% Write a program that uses set subtraction to find words in the book
%ğŸ% that are not in the word list.  Solution:
%ğŸ% \url{http://thinkpython2.com/code/analyze_book2.py}.

Pythonã€€æä¾›äº†ä¸€ä¸ªå«åšé›†åˆ (set) çš„æ•°æ®ç»“æ„ï¼Œ æ”¯æŒè®¸å¤šå¸¸è§çš„é›†åˆæ“ä½œã€‚
ä½ å¯ä»¥å‰å¾€ç¬¬åä¹ç« é˜…è¯»ç›¸å…³å†…å®¹ï¼Œ æˆ–è€…é˜…è¯»\href{http://docs.python.org/3/library/stdtypes.html#types-set}{å®˜æ–¹æ–‡æ¡£} ã€‚

ç¼–å†™ä¸€ä¸ªç¨‹åºï¼Œ ä½¿ç”¨é›†åˆçš„å·®é›†æ“ä½œæ¥æ‰¾å‡ºä¸€æœ¬ä¹¦ä¸­ä¸åœ¨ \li{work list} ä¸­çš„å•è¯ã€‚

\href{http://thinkpython2.com/code/analyze_book2.py}{å‚è€ƒç­”æ¡ˆ}

\end{exercise}

%ğŸ% \section{Random words  |  éšæœºå•è¯}
\section{éšæœºå•è¯}
\label{randomwords}
\index{histogram!random choice}

%ğŸ% To choose a random word from the histogram, the simplest algorithm
%ğŸ% is to build a list with multiple copies of each word, according
%ğŸ% to the observed frequency, and then choose from the list:

å¦‚æœæƒ³ä»ç›´æ–¹å›¾ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå•è¯ï¼Œ æœ€ç®€å•çš„ç®—æ³•æ˜¯åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼Œ
å…¶ä¸­æ ¹æ®å…¶å‡ºç°çš„é¢‘ç‡ï¼Œ æ¯ä¸ªå•è¯éƒ½æœ‰ç›¸åº”ä¸ªæ•°çš„æ‹·è´ï¼Œ ç„¶åä»è¯¥åˆ—è¡¨ä¸­é€‰æ‹©å•è¯ï¼š

\begin{lstlisting}
def random_word(h):
    t = []
    for word, freq in h.items():
        t.extend([word] * freq)
    return random.choice(t)
\end{lstlisting}

%ğŸ% %
%ğŸ% The expression {\tt [word] * freq} creates a list with {\tt freq}
%ğŸ% copies of the string {\tt word}.  The {\tt extend}
%ğŸ% method is similar to {\tt append} except that the argument is
%ğŸ% a sequence.

è¡¨è¾¾å¼ \li{[word] * freq} åˆ›å»ºä¸€ä¸ªå…·æœ‰ \li{freq} ä¸ª \li{word} å­—ç¬¦ä¸²æ‹·è´çš„åˆ—è¡¨ã€‚
é™¤äº†å®ƒçš„å®å‚è¦æ±‚æ˜¯ä¸€ä¸ªåºåˆ—å¤–ï¼Œ \li{extend} æ–¹æ³•å’Œ \li{append} æ–¹æ³•å¾ˆåƒã€‚

%ğŸ% This algorithm works, but it is not very efficient; each time you
%ğŸ% choose a random word, it rebuilds the list, which is as big as
%ğŸ% the original book.  An obvious improvement is to build the list
%ğŸ% once and then make multiple selections, but the list is still big.
%ğŸ% An alternative is:

è¯¥ç®—æ³•èƒ½å¤Ÿæ»¡è¶³è¦æ±‚ï¼Œ ä½†æ˜¯æ•ˆç‡ä¸å¤Ÿé«˜ï¼›
æ¯æ¬¡ä½ é€‰æ‹©ä¸€ä¸ªéšæœºå•è¯ï¼Œ å®ƒéƒ½é‡å»ºåˆ—è¡¨ï¼Œ è¿™ä¸ªåˆ—è¡¨å’ŒåŸä¹¦ä¸€æ ·å¤§ã€‚
ä¸€ä¸ªæ˜æ˜¾çš„æ”¹è¿›æ˜¯ï¼Œ åˆ›å»ºåˆ—è¡¨ä¸€æ¬¡ï¼Œ ç„¶åè¿›è¡Œå¤šæ¬¡é€‰æ‹©ï¼Œ  ä½†æ˜¯è¯¥åˆ—è¡¨ä»ç„¶å¾ˆå¤§ã€‚

\begin{enumerate}
%ğŸ% \item Use {\tt keys} to get a list of the words in the book.

\item ä½¿ç”¨ \li{keys} æ¥è·å¾—è¯¥ä¹¦ä¸­å•è¯çš„åˆ—è¡¨ã€‚

%ğŸ% \item Build a list that contains the cumulative sum of the word
%ğŸ%   frequencies (see Exercise~\ref{cumulative}).  The last item
%ğŸ%   in this list is the total number of words in the book, $n$.

\item åˆ›å»ºä¸€ä¸ªåŒ…å«å•è¯é¢‘ç‡ç´¯ç§¯å’Œçš„åˆ—è¡¨(è§ ç»ƒä¹ ~\ref{cumulative})ã€‚    æ­¤åˆ—è¡¨çš„æœ€åä¸€é¡¹æ˜¯ä¹¦ä¸­å•è¯çš„æ•°ç›® $n$ ã€‚

%ğŸ% \item Choose a random number from 1 to $n$.  Use a bisection search
%ğŸ%   (See Exercise~\ref{bisection}) to find the index where the random
%ğŸ%   number would be inserted in the cumulative sum.

\item é€‰æ‹©ä¸€ä¸ªä» 1 åˆ° $n$ çš„éšæœºæ•°ã€‚   ä½¿ç”¨äºŒåˆ†æœç´¢(è§\ref{bisection}èŠ‚)  æ‰¾åˆ°è¯¥éšæœºæ•°åº”è¯¥è¢«åœ¨ç´¯ç§¯å’Œä¸­æ’å…¥çš„ç´¢å¼•ã€‚

%ğŸ% \item Use the index to find the corresponding word in the word list.

\item ä½¿ç”¨è¯¥ç´¢å¼•ä»å•è¯åˆ—è¡¨ä¸­æ‰¾åˆ°ç›¸åº”çš„å•è¯ã€‚
\end{enumerate}

\begin{exercise}
\label{randhist}
\index{algorithm}
%ğŸ% Write a program that uses this algorithm to choose a random word from
%ğŸ% the book.  Solution:
%ğŸ% \url{http://thinkpython2.com/code/analyze_book3.py}.
\end{exercise}

ç¼–å†™ä¸€ä¸ªä½¿ç”¨è¯¥ç®—æ³•ä»ä¹¦ä¸­é€‰æ‹©ä¸€ä¸ªéšæœºå•è¯çš„ç¨‹åºã€‚
\href{http://thinkpython2.com/code/analyze_book3.py}{å‚è€ƒç­”æ¡ˆ}


%ğŸ% \section{Markov analysis  |  é©¬å°”ç§‘å¤«åˆ†æ}
\section{é©¬å°”ç§‘å¤«åˆ†æ}
\label{markov}
\index{Markov analysis}

%ğŸ% If you choose words from the book at random, you can get a
%ğŸ% sense of the vocabulary, but you probably won't get a sentence:

å¦‚æœä½ ä»ä¹¦ä¸­éšæœºé€‰æ‹©å•è¯ï¼Œ é‚£ä¹ˆä½ ä¼šå¤§è‡´äº†è§£å…¶ä½¿ç”¨çš„è¯æ±‡ï¼Œ ä½†å¯èƒ½ä¸ä¼šå¾—åˆ°ä¸€ä¸ªå®Œæ•´çš„å¥å­ï¼š

\begin{lstlisting}
this the small regard harriet which knightley's it most things
\end{lstlisting}

%ğŸ% %
%ğŸ% A series of random words seldom makes sense because there
%ğŸ% is no relationship between successive words.  For example, in
%ğŸ% a real sentence you would expect an article like ``the'' to
%ğŸ% be followed by an adjective or a noun, and probably not a verb
%ğŸ% or adverb.

ä¸€ç³»åˆ—éšæœºå•è¯å¾ˆå°‘æœ‰æ„ä¹‰ï¼Œ å› ä¸ºç›¸é‚»çš„å•è¯ä¹‹é—´æ²¡æœ‰å…³ç³»ã€‚
ä¾‹å¦‚ï¼Œ åœ¨ä¸€ä¸ªçœŸå®çš„å¥å­ä¸­ï¼Œ ä½ å¯èƒ½æœŸæœ› ``the'' è¿™æ ·çš„å† è¯åé¢è·Ÿç€çš„æ˜¯ä¸€ä¸ªå½¢å®¹è¯æˆ–è€…åè¯ï¼Œ
è€Œå¤§ä¸å¯èƒ½ä¼šæ˜¯ä¸€ä¸ªåŠ¨è¯æˆ–è€…å‰¯è¯ã€‚

%ğŸ% One way to measure these kinds of relationships is Markov
%ğŸ% analysis, which
%ğŸ% characterizes, for a given sequence of words, the probability of the
%ğŸ% words that might come next.  For example, the song {\em Eric, the Half a
%ğŸ%   Bee} begins:

è¡¡é‡ç›¸é‚»å•è¯å…³ç³»çš„æ–¹æ³•ä¹‹ä¸€æ˜¯é©¬å°”ç§‘å¤«åˆ†ææ³•ï¼Œ å¯¹äºä¸€ä¸ªç»™å®šçš„å•è¯åºåˆ—ï¼Œ
é©¬å°”ç§‘å¤«åˆ†ææ³•å°†ç»™å‡ºæ¥ä¸‹æ¥å•è¯çš„æ¦‚ç‡ã€‚   ä¾‹å¦‚ï¼Œ æ­Œæ›² {\em Eric, the Half a
Bee} çš„å¼€å¤´æ˜¯ï¼š

\begin{quote}
Half a bee, philosophically, \\
Must, ipso facto, half not be. \\
But half the bee has got to be \\
Vis a vis, its entity. D'you see? \\
\\
But can a bee be said to be \\
Or not to be an entire bee \\
When half the bee is not a bee \\
Due to some ancient injury? \\
\end{quote}

%ğŸ% %
%ğŸ% In this text,
%ğŸ% the phrase ``half the'' is always followed by the word ``bee'',
%ğŸ% but the phrase ``the bee'' might be followed by either
%ğŸ% ``has'' or ``is''.

åœ¨æ­¤æ–‡æœ¬ä¸­ï¼Œ çŸ­è¯­ ``half the'' åé¢æ€»æ˜¯è·Ÿç€å•è¯ ``bee''ï¼Œ  ä½†æ˜¯çŸ­è¯­ ``the
bee'' åˆ™å¯èƒ½è·Ÿç€ ``has'' æˆ–è€…  ``is''ã€‚

\index{prefix}
\index{suffix}
\index{mapping}

%ğŸ% The result of Markov analysis is a mapping from each prefix
%ğŸ% (like ``half the'' and ``the bee'') to all possible suffixes
%ğŸ% (like ``has'' and ``is'').

é©¬å°”ç§‘å¤«åˆ†æçš„ç»“æœæ˜¯ä»æ¯ä¸ªå‰ç¼€(å¦‚``half the''å’Œ``the bee'')
åˆ°æ‰€æœ‰å¯èƒ½çš„åç¼€(å¦‚``has'' å’Œ ``is'')çš„æ˜ å°„ã€‚

\index{random text}
\index{text!random}

%ğŸ% Given this mapping, you can generate a random text by
%ğŸ% starting with any prefix and choosing at random from the
%ğŸ% possible suffixes.  Next, you can combine the end of the
%ğŸ% prefix and the new suffix to form the next prefix, and repeat.
%ğŸ%
%ğŸ% For example, if you start with the prefix ``Half a'', then the
%ğŸ% next word has to be ``bee'', because the prefix only appears
%ğŸ% once in the text.  The next prefix is ``a bee'', so the
%ğŸ% next suffix might be ``philosophically'', ``be'' or ``due''.
%ğŸ%
%ğŸ% In this example the length of the prefix is always two, but
%ğŸ% you can do Markov analysis with any prefix length.

ç»™å®šæ­¤æ˜ å°„ï¼Œ ä½ èƒ½å¤Ÿé€šè¿‡ä»¥ä»»æ„å‰ç¼€å¼€å§‹å¹¶ä»å¯èƒ½çš„åç¼€ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªçš„æ–¹æ³•ï¼Œ æ¥ç”Ÿæˆä¸€ä¸ªéšæœºæ–‡æœ¬ã€‚
æ¥ä¸‹æ¥ï¼Œ ä½ å¯ä»¥å°†å‰ç¼€çš„ç»“å°¾å’Œæ–°çš„åç¼€ç»„åˆæˆä¸‹ä¸€ä¸ªå‰ç¼€ï¼Œ å¹¶é‡å¤ä¸‹å»ã€‚

ä¾‹å¦‚ï¼Œ å¦‚æœä½ ä»¥å‰ç¼€``Half a''å¼€å§‹ï¼Œ ç„¶åä¸‹ä¸€ä¸ªä½†æ˜¯å¿…é¡»æ˜¯``bee''ï¼Œ
å› ä¸ºæ­¤å‰ç¼€åœ¨æ–‡æœ¬ä¸­ä»…å‡ºç°ä¸€æ¬¡ã€‚  ä¸‹ä¸€ä¸ªå‰ç¼€æ˜¯``a bee''ï¼Œ
æ‰€ä»¥ä¸‹ä¸€ä¸ªåç¼€å¯èƒ½æ˜¯``philosophically''ï¼Œ ``be''æˆ–``due''ã€‚

æ­¤ä¾‹ä¸­ï¼Œ å‰ç¼€çš„é•¿åº¦æ€»æ˜¯2ï¼Œ ä½†æ˜¯ä½ å¯ä»¥ä»¥ä»»æ„å‰ç¼€é•¿åº¦è¿›è¡Œé©¬å°”ç§‘å¤«åˆ†æã€‚
å‰ç¼€çš„é•¿åº¦è¢«ç§°ä½œæ­¤åˆ†æçš„â€œé˜¶â€ã€‚

\begin{exercise}
%ğŸ% Markov analysis:
é©¬å°”ç§‘å¤«åˆ†æï¼š
\begin{enumerate}
%ğŸ% \item Write a program to read a text from a file and perform Markov
%ğŸ% analysis.  The result should be a dictionary that maps from
%ğŸ% prefixes to a collection of possible suffixes.  The collection
%ğŸ% might be a list, tuple, or dictionary; it is up to you to make
%ğŸ% an appropriate choice.  You can test your program with prefix
%ğŸ% length two, but you should write the program in a way that makes
%ğŸ% it easy to try other lengths.

\item ç¼–å†™ä¸€ä¸ªç¨‹åºï¼Œ ä»ä¸€ä¸ªæ–‡ä»¶ä¸­è¯»å–æ–‡æœ¬å¹¶æ‰§è¡Œé©¬å°”ç§‘å¤«åˆ†æã€‚
   ç»“æœåº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œ å³ä»å‰ç¼€æ˜ å°„åˆ°ä¸€ä¸ªå¯èƒ½çš„åç¼€é›†åˆã€‚
   æ­¤åç¼€é›†åˆå¯ä»¥æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ï¼›ä½ éœ€è¦åšå‡ºåˆé€‚çš„é€‰æ‹©ã€‚
   ä½ å¯ä»¥ç”¨é•¿åº¦ä¸º2çš„å‰ç¼€æµ‹è¯•ç¨‹åºï¼Œ ä½†æ˜¯åœ¨ç¼–å†™ç¨‹åºæ—¶ï¼Œ åº”ç¡®ä¿å…¶å¾ˆå®¹æ˜“æ”¯æŒå…¶å®ƒé•¿åº¦ã€‚

%ğŸ% \item Add a function to the previous program to generate random text
%ğŸ% based on the Markov analysis.  Here is an example from {\em Emma}
%ğŸ% with prefix length 2:
%ğŸ%
%ğŸ% \begin{quote}
%ğŸ% He was very clever, be it sweetness or be angry, ashamed or only
%ğŸ% amused, at such a stroke. She had never thought of Hannah till you
%ğŸ% were never meant for me?" "I cannot make speeches, Emma:" he soon cut
%ğŸ% it all himself.
%ğŸ% \end{quote}
%ğŸ%
%ğŸ% For this example, I left the punctuation attached to the words.
%ğŸ% The result is almost syntactically correct, but not quite.
%ğŸ% Semantically, it almost makes sense, but not quite.
%ğŸ%
%ğŸ% What happens if you increase the prefix length?  Does the random
%ğŸ% text make more sense?

\item åœ¨å‰é¢çš„ç¨‹åºä¸­æ·»åŠ ä¸€ä¸ªå‡½æ•°ï¼Œ åŸºäºé©¬å°”ç§‘å¤«åˆ†æç”Ÿæˆéšæœºæ–‡æœ¬ã€‚
   ä¸‹é¢æ˜¯ä½¿ç”¨ ã€Š{\em Emma} ã€‹ æ‰§è¡Œå‰ç¼€ä¸º2çš„é©¬å°”ç§‘å¤«åˆ†æç”Ÿæˆçš„ç¤ºä¾‹ï¼š

\begin{quote}
He was very clever, be it sweetness or be angry, ashamed or only
amused, at such a stroke. She had never thought of Hannah till you
were never meant for me?" "I cannot make speeches, Emma:" he soon cut
it all himself.
\end{quote}

åœ¨æ­¤ä¾‹ä¸­ï¼Œ æˆ‘ä¿ç•™äº†é™„åœ¨è¯åé¢çš„æ ‡ç‚¹ç¬¦å·ã€‚  ä»è¯­æ³•ä¸Šçœ‹ï¼Œ ç»“æœå‡ ä¹æ˜¯æ­£ç¡®çš„ï¼Œ ä½†ä¸å®Œå…¨ã€‚
è¯­ä¹‰ä¸Šè®²ï¼Œ å®ƒå‡ ä¹æœ‰æ„ä¹‰ï¼Œ ä½†ä¹Ÿä¸å®Œå…¨ã€‚

å¦‚æœä½ å¢åŠ å‰ç¼€çš„é•¿åº¦ï¼Œ ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿéšæœºæ–‡æœ¬æ›´æœ‰æ„ä¹‰æ˜¯ä¹ˆï¼Ÿ

%ğŸ% \item Once your program is working, you might want to try a mash-up:
%ğŸ% if you combine text from two or more books, the random
%ğŸ% text you generate will blend the vocabulary and phrases from
%ğŸ% the sources in interesting ways.

\item ä¸€æ—¦ç¨‹åºæ­£å¸¸è¿è¡Œï¼Œ ä½ å¯ä»¥æƒ³å°è¯•ä¸€ä¸‹æ··æ­ï¼šå¦‚æœä½ ç»„åˆä¸¤æœ¬æˆ–æ›´å¤šä¹¦ä¸­çš„æ–‡æœ¬ï¼Œ
   ä½ ç”Ÿæˆçš„éšæœºæ–‡æœ¬å°†ä»¥æœ‰è¶£çš„æ–¹å¼æ··åˆè¿™äº›ä¹¦ä¸­çš„è¯æ±‡å’ŒçŸ­è¯­ã€‚

\index{mash-up}

\end{enumerate}

%ğŸ% Credit: This case study is based on an example from Kernighan and
%ğŸ% Pike, {\em The Practice of Programming}, Addison-Wesley, 1999.

è‡´è°¢ï¼šæ­¤æ¡ˆä¾‹ç ”ç©¶åŸºäº Kernighan ä¸ Pike æ‰€è‘—çš„ ã€Š{\em The Practice of
Programming}ã€‹ ä¸€ä¹¦ä¸­çš„ç¤ºä¾‹ã€‚

\end{exercise}

%ğŸ% You should attempt this exercise before you go on; then you can can
%ğŸ% download my solution from \url{http://thinkpython2.com/code/markov.py}.
%ğŸ% You will also need \url{http://thinkpython2.com/code/emma.txt}.

åœ¨ç»§ç»­é˜…è¯»ä¹‹å‰ï¼Œ ä½ åº”è¯¥å°è¯•è§£å†³è¿™ä¸ªä¹ é¢˜ï¼›
ä½ å¯ä»¥ä» \href{http://thinkpython2.com/code/markov.py}{æ­¤å¤„}ä¸‹è½½æˆ‘çš„ç­”æ¡ˆã€‚
ä½ è¿˜éœ€è¦ä¸‹è½½\href{http://thinkpython2.com/code/emma.txt}{Emma çš„æ–‡æœ¬} ã€‚

%ğŸ% \section{Data structures  |  æ•°æ®ç»“æ„}
\section{æ•°æ®ç»“æ„}
\index{data structure}

%ğŸ% Using Markov analysis to generate random text is fun, but there is
%ğŸ% also a point to this exercise: data structure selection.  In your
%ğŸ% solution to the previous exercises, you had to choose:

ä½¿ç”¨é©¬å°”ç§‘å¤«åˆ†æç”Ÿæˆéšæœºæ–‡æœ¬å¾ˆæœ‰è¶£ï¼Œ
ä½†æ˜¯ä¸Šé¢é‚£é“ä¹ é¢˜çš„ç›®çš„æ˜¯ï¼šå­¦ä¹ æ•°æ®ç»“æ„é€‰æ‹©ã€‚
åœ¨è§£ç­”ä¸Šè¿°ä¹ é¢˜æ—¶ï¼Œ ä½ ä¸å¾—ä¸é€‰æ‹©ï¼š

%ğŸ% \begin{itemize}
%ğŸ% \item How to represent the prefixes.
%ğŸ% \item How to represent the collection of possible suffixes.
%ğŸ% \item How to represent the mapping from each prefix to
%ğŸ% the collection of possible suffixes.
%ğŸ% \end{itemize}

\begin{itemize}
\item å¦‚ä½•è¡¨ç¤ºå‰ç¼€ã€‚
\item å¦‚ä½•è¡¨ç¤ºå¯èƒ½åç¼€çš„é›†åˆã€‚
\item å¦‚ä½•è¡¨ç¤ºä»å‰ç¼€åˆ°å¯èƒ½åç¼€é›†åˆçš„æ˜ å°„ã€‚
\end{itemize}

%ğŸ% The last one is easy: a dictionary is the obvious choice
%ğŸ% for a mapping from keys to corresponding values.

æœ€åä¸€ä¸ªé€‰æ‹©å¾ˆç®€å•ï¼šæ˜æ˜¾åº”è¯¥é€‰æ‹©å­—å…¸ä½œä¸ºé”®è‡³å¯¹åº”å€¼çš„æ˜ å°„ã€‚

%ğŸ% For the prefixes, the most obvious options are string,
%ğŸ% list of strings, or tuple of strings.

å¯¹äºå‰ç¼€ï¼Œ æœ€æ˜æ˜¾çš„é€‰æ‹©æ˜¯å­—ç¬¦ä¸²ã€å­—ç¬¦ä¸²åˆ—è¡¨æˆ–è€…å­—ç¬¦ä¸²å…ƒç»„ã€‚

%ğŸ% For the suffixes,
%ğŸ% one option is a list; another is a histogram (dictionary).

å¯¹äºåç¼€ï¼Œ ä¸€ä¸ªé€‰æ‹©æ˜¯åˆ—è¡¨ï¼›å¦ä¸€ä¸ªæ˜¯ç›´æ–¹å›¾(å­—å…¸)ã€‚

\index{implementation}

%ğŸ% How should you choose?  The first step is to think about
%ğŸ% the operations you will need to implement for each data structure.
%ğŸ% For the prefixes, we need to be able to remove words from
%ğŸ% the beginning and add to the end.  For example, if the current
%ğŸ% prefix is ``Half a'', and the next word is ``bee'', you need
%ğŸ% to be able to form the next prefix, ``a bee''.

ä½ å¦‚ä½•é€‰æ‹©å‘¢ï¼Ÿ ç¬¬ä¸€æ­¥æ˜¯è€ƒè™‘å¯¹æ¯ä¸ªæ•°æ®ç»“æ„ä½ éœ€è¦å®ç°çš„æ“ä½œã€‚
å¯¹äºå‰ç¼€ï¼Œ æˆ‘ä»¬éœ€è¦èƒ½ä»å¤´éƒ¨åˆ é™¤å•è¯ï¼Œ å¹¶åœ¨ç»“å°¾å¤„åŠ å…¥å•è¯ã€‚
ä¾‹å¦‚ï¼Œ å¦‚æœå½“å‰çš„å‰ç¼€æ˜¯``Half a''ï¼Œ ä¸‹ä¸€ä¸ªè¯æ˜¯``bee''ï¼Œ
ä½ éœ€è¦èƒ½æ„æˆä¸‹ä¸€ä¸ªå‰ç¼€``a bee''ã€‚
\index{tuple!as key in dictionary}

%ğŸ% Your first choice might be a list, since it is easy to add
%ğŸ% and remove elements, but we also need to be able to use the
%ğŸ% prefixes as keys in a dictionary, so that rules out lists.
%ğŸ% With tuples, you can't append or remove, but you can use
%ğŸ% the addition operator to form a new tuple:

ä½ çš„ç¬¬ä¸€ä¸ªé€‰æ‹©å¯èƒ½æ˜¯åˆ—è¡¨ï¼Œ å› ä¸ºå®ƒèƒ½å¾ˆå®¹æ˜“çš„å¢åŠ å’Œåˆ é™¤å…ƒç´ ï¼Œ
ä½†æ˜¯æˆ‘ä»¬ä¹Ÿéœ€è¦è®©å‰ç¼€ä½œä¸ºå­—å…¸çš„é”®ï¼Œ è¿™å°±æ’é™¤äº†åˆ—è¡¨ã€‚
ä½¿ç”¨å…ƒç»„ï¼Œ ä½ ä¸èƒ½è¿½åŠ æˆ–åˆ é™¤å…ƒç´ ï¼Œ
ä½†æ˜¯ä½ èƒ½ä½¿ç”¨åŠ å·è¿ç®—ç¬¦æ¥å½¢æˆä¸€ä¸ªæ–°çš„å…ƒç»„ï¼š

\begin{lstlisting}
def shift(prefix, word):
    return prefix[1:] + (word,)
\end{lstlisting}

%ğŸ% %
%ğŸ% {\tt shift} takes a tuple of words, {\tt prefix}, and a string,
%ğŸ% {\tt word}, and forms a new tuple that has all the words
%ğŸ% in {\tt prefix} except the first, and {\tt word} added to
%ğŸ% the end.

\li{shift} æ¥å—ä¸€ä¸ªå•è¯å…ƒç»„ \li{prefix} å’Œä¸€ä¸ªå­—ç¬¦ä¸² \li{word} ï¼Œ
å¹¶å½¢æˆä¸€ä¸ªæ–°çš„å…ƒç»„ï¼Œ å…¶å…·æœ‰ \li{prefix} ä¸­é™¤ç¬¬ä¸€ä¸ªå•è¯å¤–çš„å…¨éƒ¨å•è¯ï¼Œ
ç„¶ååœ¨ç»“å°¾å¢åŠ  \li{word} ã€‚

%ğŸ% For the collection of suffixes, the operations we need to
%ğŸ% perform include adding a new suffix (or increasing the frequency
%ğŸ% of an existing one), and choosing a random suffix.

å¯¹äºåç¼€çš„é›†åˆï¼Œ æˆ‘ä»¬éœ€è¦æ‰§è¡Œçš„è¿ç®—åŒ…æ‹¬å¢åŠ ä¸€ä¸ªæ–°çš„åç¼€
(æˆ–è€…å¢åŠ ä¸€ä¸ªå·²æœ‰åç¼€çš„é¢‘ç‡)ï¼Œ å¹¶é€‰æ‹©ä¸€ä¸ªéšæœºåç¼€ã€‚

%ğŸ% Adding a new suffix is equally easy for the list implementation
%ğŸ% or the histogram.  Choosing a random element from a list
%ğŸ% is easy; choosing from a histogram is harder to do
%ğŸ% efficiently (see Exercise~\ref{randhist}).

å¯¹äºåˆ—è¡¨æˆ–è€…ç›´æ–¹å›¾ï¼Œ å¢åŠ ä¸€ä¸ªæ–°çš„åç¼€ä¸€æ ·å®¹æ˜“ã€‚
ä»åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªéšæœºå…ƒç´ å¾ˆå®¹æ˜“ï¼›
åœ¨ç›´æ–¹å›¾ä¸­é€‰æ‹©çš„éš¾åº¦æ›´å¤§(è§ ç»ƒä¹ ~\ref{randhist})ã€‚

%ğŸ% So far we have been talking mostly about ease of implementation,
%ğŸ% but there are other factors to consider in choosing data structures.
%ğŸ% One is run time.  Sometimes there is a theoretical reason to expect
%ğŸ% one data structure to be faster than other; for example, I mentioned
%ğŸ% that the {\tt in} operator is faster for dictionaries than for lists,
%ğŸ% at least when the number of elements is large.

ç›®å‰ä¸ºæ­¢ï¼Œ æˆ‘ä»¬ä¸»è¦è®¨è®ºå®ç°çš„éš¾æ˜“ï¼Œ
ä½†æ˜¯é€‰æ‹©æ•°æ®ç»“æ„æ—¶è¿˜è¦è€ƒè™‘å…¶å®ƒå› ç´ ã€‚  ä¸€ä¸ªæ˜¯è¿è¡Œæ—¶é—´ã€‚
æœ‰æ—¶ï¼Œ ä¸€ä¸ªæ•°æ®ç»“æ„æ¯”å¦ä¸€ä¸ªå¿«æœ‰ç†è®ºä¾æ®ï¼›
ä¾‹å¦‚ï¼Œ æˆ‘æåˆ°è¿‡ \li{{in} è¿ç®—ç¬¦å¯¹äºå­—å…¸æ¯”å¯¹åˆ—è¡¨è¦å¿«ï¼Œ
è‡³å°‘å½“å…ƒç´ çš„æ•°ç›®å¾ˆå¤§çš„æ—¶å€™ã€‚

%ğŸ% But often you don't know ahead of time which implementation will
%ğŸ% be faster.  One option is to implement both of them and see which
%ğŸ% is better.  This approach is called {\bf benchmarking}.  A practical
%ğŸ% alternative is to choose the data structure that is
%ğŸ% easiest to implement, and then see if it is fast enough for the
%ğŸ% intended application.  If so, there is no need to go on.  If not,
%ğŸ% there are tools, like the {\tt profile} module, that can identify
%ğŸ% the places in a program that take the most time.

ä½†æ˜¯é€šå¸¸ä½ äº‹å…ˆä¸çŸ¥é“å“ªä¸ªå®ç°æ›´å¿«ã€‚
ä¸€ä¸ªé€‰æ‹©æ˜¯ä¸¤ä¸ªéƒ½å®ç°ï¼Œ ç„¶åå†çœ‹å“ªä¸ªæ›´å¿«ã€‚
æ­¤æ–¹æ³•è¢«ç§°ä½œ {\em åŸºå‡†æµ‹è¯•} (benchmarking) ã€‚
å¦ä¸€ä¸ªæ›´å®é™…çš„é€‰æ‹©æ˜¯é€‰æ‹©æœ€å®¹æ˜“å®ç°çš„æ•°æ®ç»“æ„ï¼Œ
ç„¶åçœ‹å®ƒå¯¹äºæ‹Ÿå®šçš„åº”ç”¨æ˜¯å¦è¶³å¤Ÿå¿«ã€‚  å¦‚æœæ˜¯çš„è¯ï¼Œ å°±ä¸éœ€è¦ç»§ç»­äº†ã€‚
å¦‚æœä¸æ˜¯ï¼Œ å¯ä»¥ä½¿ç”¨ä¸€äº›å·¥å…·ï¼Œ å¦‚ \li{profile} æ¨¡å—ï¼Œ è¯†åˆ«ç¨‹åºä¸­å“ªå¤„æœ€è€—æ—¶ã€‚
\index{benchmarking}
\index{profile module}
\index{module!profile}

%ğŸ% The other factor to consider is storage space.  For example, using a
%ğŸ% histogram for the collection of suffixes might take less space because
%ğŸ% you only have to store each word once, no matter how many times it
%ğŸ% appears in the text.  In some cases, saving space can also make your
%ğŸ% program run faster, and in the extreme, your program might not run at
%ğŸ% all if you run out of memory.  But for many applications, space is a
%ğŸ% secondary consideration after run time.

å¦ä¸€ä¸ªè¦è€ƒè™‘çš„å› ç´ æ˜¯å­˜å‚¨ç©ºé—´ã€‚  ä¾‹å¦‚ï¼Œ ä½¿ç”¨ç›´æ–¹å›¾è¡¨ç¤ºåç¼€é›†åˆå¯èƒ½ç”¨æ›´å°‘çš„ç©ºé—´ï¼Œ
å› ä¸ºæ— è®ºä¸€ä¸ªå•è¯åœ¨æ–‡æœ¬ä¸­å‡ºç°å¤šå°‘æ¬¡ï¼Œ ä½ åªéœ€è¦å­˜å‚¨å®ƒä¸€æ¬¡ã€‚
åœ¨ä¸€äº›æƒ…å†µä¸‹ï¼Œ èŠ‚çœç©ºé—´ä¹Ÿèƒ½è®©ä½ çš„ç¨‹åºæ›´å¿«ï¼Œ æç«¯æƒ…å†µä¸‹ï¼Œ
å¦‚æœå†…å­˜æº¢å‡ºï¼Œ ä½ çš„ç¨‹åºå¯èƒ½æ ¹æœ¬ä¸èƒ½è¿è¡Œã€‚
ä½†æ˜¯å¯¹äºè®¸å¤šåº”ç”¨ï¼Œ ç©ºé—´æ˜¯è¿è¡Œæ—¶é—´ä¹‹åçš„ç¬¬äºŒä½è€ƒè™‘ã€‚

%ğŸ% One final thought: in this discussion, I have implied that
%ğŸ% we should use one data structure for both analysis and generation.  But
%ğŸ% since these are separate phases, it would also be possible to use one
%ğŸ% structure for analysis and then convert to another structure for
%ğŸ% generation.  This would be a net win if the time saved during
%ğŸ% generation exceeded the time spent in conversion.

æœ€åä¸€ç‚¹ï¼šåœ¨æ­¤è®¨è®ºä¸­ï¼Œ æˆ‘æš—ç¤ºäº†æˆ‘ä»¬åº”è¯¥ä½¿ç”¨ä¸€ç§æ•°æ®ç»“æ„åŒæ—¶è¿›è¡Œåˆ†æå’Œç”Ÿæˆã€‚
ä½†æ˜¯æ—¢ç„¶è¿™äº›æ˜¯ç‹¬ç«‹çš„æ­¥éª¤ï¼Œ ä½¿ç”¨ä¸€ç§æ•°æ®ç»“æ„è¿›è¡Œåˆ†æï¼Œ
ç„¶åé‡‡ç”¨å¦ä¸€ç§ç»“æ„è¿›è¡Œç”Ÿæˆä¹Ÿæ˜¯å¯èƒ½çš„ã€‚
å¦‚æœç”ŸæˆèŠ‚çœçš„æ—¶é—´è¶…è¿‡äº†è½¬åŒ–èŠ±è´¹çš„æ—¶é—´ï¼Œ è¿™ä¹Ÿä¼šæé«˜ç¨‹åºçš„æ€§èƒ½ã€‚

%ğŸ% \section{Debugging  |  è°ƒè¯•}
\section{è°ƒè¯•}
\index{debugging}
%ğŸ% When you are debugging a program, and especially if you are
%ğŸ% working on a hard bug, there are five things to try:

åœ¨è°ƒè¯•ä¸€ä¸ªç¨‹åºçš„æ—¶å€™ï¼Œ ç‰¹åˆ«æ˜¯è°ƒè¯•ä¸€ä¸ªå¾ˆéš¾çš„é”™è¯¯æ—¶ï¼Œ åº”è¯¥åšåˆ°ä»¥ä¸‹äº”ç‚¹ï¼š

%ğŸ% \begin{description}
%ğŸ% \item[Reading:] Examine your code, read it back to yourself, and
%ğŸ% check that it says what you meant to say.
%ğŸ% \item[Running:] Experiment by making changes and running different
%ğŸ% versions.  Often if you display the right thing at the right place
%ğŸ% in the program, the problem becomes obvious, but sometimes you have to
%ğŸ% build scaffolding.
%ğŸ% \item[Ruminating:] Take some time to think!  What kind of error
%ğŸ% is it: syntax, runtime, or semantic?  What information can you get from
%ğŸ% the error messages, or from the output of the program?  What kind of
%ğŸ% error could cause the problem you're seeing?  What did you change
%ğŸ% last, before the problem appeared?
%ğŸ% \item[Rubberducking:] If you explain the problem to someone else, you
%ğŸ%   sometimes find the answer before you finish asking the question.
%ğŸ%   Often you don't need the other person; you could just talk to a rubber
%ğŸ%   duck.  And that's the origin of the well-known strategy called {\bf
%ğŸ%     rubber duck debugging}.  I am not making this up; see
%ğŸ%   \url{https://en.wikipedia.org/wiki/Rubber_duck_debugging}.
%ğŸ% \item[Retreating:] At some point, the best thing to do is back
%ğŸ% off, undoing recent changes, until you get back to a program that
%ğŸ% works and that you understand.  Then you can start rebuilding.
%ğŸ% \end{description}

\begin{description}

\item [ç»†è¯»ï¼š]æ£€æŸ¥ä½ çš„ä»£ç ï¼Œ ä»”ç»†åœ°é˜…è¯»ï¼Œ å¹¶ä¸”æ£€æŸ¥æ˜¯å¦å®ç°äº†ä½ çš„æœŸæœ›ã€‚

\item [è¿è¡Œï¼š]é€šè¿‡ä¿®æ”¹å’Œè¿è¡Œä¸åŒçš„ç‰ˆæœ¬æ¥ä¸æ–­è¯•éªŒã€‚    é€šå¸¸ï¼Œ å¦‚æœä½ åœ¨ç¨‹åºä¸­æ­£ç¡®çš„åœ°æ–¹æ‰“å°äº†æ­£ç¡®çš„ä¸œè¥¿ï¼Œ é—®é¢˜ä¼šå˜å¾—å¾ˆæ˜æ˜¾ï¼Œ ä½†æ˜¯æœ‰æ—¶ä½ ä¸å¾—ä¸æ­å»ºä¸€äº›è„šæ‰‹æ¶ã€‚

\item [æ€è€ƒï¼š]    èŠ±äº›æ—¶é—´æ€è€ƒï¼é”™è¯¯çš„ç±»å‹æ˜¯ä»€ä¹ˆï¼šè¯­æ³•ã€è¿è¡Œæ—¶ã€è¯­ä¹‰ï¼Ÿ
    ä½ ä»é”™è¯¯ä¿¡æ¯æˆ–è€…ç¨‹åºçš„è¾“å‡ºä¸­èƒ½è·å¾—ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
    ä»€ä¹ˆç±»å‹çš„é”™è¯¯èƒ½å¼•èµ·ä½ çœ‹åˆ°çš„é—®é¢˜ï¼Ÿé—®é¢˜å‡ºç°å‰ï¼Œ ä½ æœ€åçš„ä¿®æ”¹æ˜¯ä»€ä¹ˆï¼Ÿ

\item [å°é»„é¸­è°ƒè¯•æ³•(rubberducking)ï¼š]å¦‚æœå°†ä½ çš„é—®é¢˜è§£é‡Šç»™åˆ«äººå¬ï¼Œ æœ‰æ—¶ä½ ä¼šå‘ç°åœ¨è§£é‡Šå®Œé—®é¢˜ä¹‹å‰å°±èƒ½æ‰¾åˆ°ç­”æ¡ˆã€‚
    ä½ é€šå¸¸å¹¶ä¸éœ€è¦çœŸçš„å»é—®å¦å¤–ä¸€ä¸ªäººï¼›ä½ å¯ä»¥å¯¹ç€ä¸€ä¸ªå°é»„é¸­è¯´ã€‚
    è¿™å°±æ˜¯è‘—åçš„{\em å°é»„é¸­è°ƒè¯•æ³•} (rubber duck
    debugging) çš„ç”±æ¥ã€‚   è¿™å¯ä¸æ˜¯æˆ‘ç¼–é€ çš„ï¼Œ çœ‹çœ‹\href{https://en.wikipedia.org/wiki/Rubber_duck_debugging}{ç»´åŸºçš„è§£é‡Š}ã€‚

\index{ç»´åŸºç™¾ç§‘}

\item [å›é€€ï¼š]æœ‰æ—¶å€™ï¼Œ æœ€å¥½çš„åšæ³•æ˜¯å›é€€ï¼Œ æ’¤é”€æœ€è¿‘çš„ä¿®æ”¹ï¼Œ
    ç›´åˆ°ä½ å›åˆ°ä¸€ä¸ªèƒ½è¿è¡Œå¹¶ä¸”ä½ èƒ½ç†è§£çš„ç¨‹åºã€‚  ç„¶åä½ å¯ä»¥å¼€å§‹é‡å»ºã€‚

\end{description}


%ğŸ% Beginning programmers sometimes get stuck on one of these activities
%ğŸ% and forget the others.  Each activity comes with its own failure
%ğŸ% mode.

åˆçº§ç¨‹åºå‘˜æœ‰æ—¶é™·å…¥è¿™äº›æ­¥éª¤ä¹‹ä¸€ï¼Œ å¿˜è®°äº†è¿˜å¯ä»¥åšå…¶ä»–çš„äº‹æƒ…ã€‚
äº‹å®ä¸Šï¼Œ æ¯ç§æ–¹æ³•éƒ½æœ‰å¤±è´¥çš„å¯èƒ½ã€‚

\index{typographical error}

%ğŸ% For example, reading your code might help if the problem is a
%ğŸ% typographical error, but not if the problem is a conceptual
%ğŸ% misunderstanding.  If you don't understand what your program does, you
%ğŸ% can read it 100 times and never see the error, because the error is in
%ğŸ% your head.

ä¾‹å¦‚ï¼Œ å¦‚æœç¨‹åºæ˜¯ä¸€ä¸ªæ’ç‰ˆé”™è¯¯ï¼Œ è¯»ä»£ç å¯èƒ½æœ‰å¸®åŠ©ï¼Œ
ä½†æ˜¯å¦‚æœé—®é¢˜æ˜¯æ¦‚å¿µç†è§£é”™è¯¯ï¼Œ åˆ™æœªå¿…æ˜¯è¿™æ ·ã€‚
å¦‚æœä½ ä¸ç†è§£ç¨‹åºè¦åšä»€ä¹ˆï¼Œ å¯èƒ½è¯»100éç¨‹åºéƒ½ä¸ä¼šå‘ç°é”™è¯¯ï¼Œ å› ä¸ºé”™è¯¯åœ¨ä½ çš„å¤´è„‘ä¸­ã€‚

\index{experimental debugging}

%ğŸ% Running experiments can help, especially if you run small, simple
%ğŸ% tests.  But if you run experiments without thinking or reading your
%ğŸ% code, you might fall into a pattern I call ``random walk programming'',
%ğŸ% which is the process of making random changes until the program
%ğŸ% does the right thing.  Needless to say, random walk programming
%ğŸ% can take a long time.

è¯•éªŒå¯èƒ½ä¼šæœ‰å¸®åŠ©ï¼Œ ç‰¹åˆ«æ˜¯å¦‚æœä½ è¿è¡Œç®€å•çŸ­å°çš„æµ‹è¯•ã€‚
ä½†æ˜¯ï¼Œ å¦‚æœä½ ä¸æ€è€ƒæˆ–è€…é˜…è¯»ä½ çš„ä»£ç ï¼Œ å°±ç›´æ¥è¿›è¡Œå®éªŒï¼Œ
ä½ å¯èƒ½é™·å…¥ä¸€ç§æˆ‘ç§°ä¸ºâ€œéšæœºæ¸¸èµ°ç¼–ç¨‹â€çš„æ¨¡å¼ã€‚
è¿™æŒ‡çš„æ˜¯éšæœºä¿®æ”¹ï¼Œ ç›´åˆ°ç¨‹åºé€šè¿‡æµ‹è¯•ã€‚
ä¸ç”¨è¯´ï¼Œ éšæœºæ¸¸èµ°ç¼–ç¨‹ä¼šèŠ±è´¹å¾ˆé•¿çš„æ—¶é—´ã€‚

\index{random walk programming}
\index{development plan!random walk programming}

%ğŸ% You have to take time to think.  Debugging is like an
%ğŸ% experimental science.  You should have at least one hypothesis about
%ğŸ% what the problem is.  If there are two or more possibilities, try to
%ğŸ% think of a test that would eliminate one of them.

ä½ å¿…é¡»èŠ±æ—¶é—´æ€è€ƒã€‚  è°ƒè¯•å°±åƒæ˜¯ä¸€é—¨å®éªŒç§‘å­¦ã€‚
ä½ åº”è¯¥è‡³å°‘æœ‰ä¸€ä¸ªå…³äºé—®é¢˜æ˜¯ä»€ä¹ˆçš„å‡è®¾ã€‚
å¦‚æœæœ‰ä¸¤ä¸ªæˆ–è€…æ›´å¤šçš„å¯èƒ½ï¼Œ è¯•ç€è€ƒè™‘åˆ©ç”¨æµ‹è¯•æ¶ˆé™¤å…¶ä¸­ä¸€ä¸ªå¯èƒ½ã€‚

%ğŸ% But even the best debugging techniques will fail if there are too many
%ğŸ% errors, or if the code you are trying to fix is too big and
%ğŸ% complicated.  Sometimes the best option is to retreat, simplifying the
%ğŸ% program until you get to something that works and that you
%ğŸ% understand.

ä½†æ˜¯ï¼Œ å¦‚æœæœ‰å¤ªå¤šçš„é”™è¯¯ï¼Œ æˆ–è€…ä½ æ­£è¯•å›¾ä¿®å¤çš„ä»£ç å¤ªå¤§ã€å¤ªå¤æ‚ï¼Œ
å³ä½¿æœ€å¥½çš„è°ƒè¯•æŠ€å·§ä¹Ÿä¼šå¤±è´¥ã€‚
æœ‰æ—¶ï¼Œ æœ€å¥½çš„é€‰æ‹©æ˜¯å›é€€ï¼Œ ç®€åŒ–ç¨‹åºï¼Œ ç›´åˆ°ä½ è·å¾—ä¸€ä¸ªæ­£å¸¸è¿è¡Œå¹¶ä¸”èƒ½ç†è§£çš„ç¨‹åºã€‚

%ğŸ% Beginning programmers are often reluctant to retreat because
%ğŸ% they can't stand to delete a line of code (even if it's wrong).
%ğŸ% If it makes you feel better, copy your program into another file
%ğŸ% before you start stripping it down.  Then you can copy the pieces
%ğŸ% back one at a time.

åˆçº§ç¨‹åºå‘˜ç»å¸¸ä¸æ„¿æ„å›é€€ï¼Œ å› ä¸ºä»–ä»¬èˆä¸å¾—åˆ é™¤ä¸€è¡Œä»£ç (å³ä½¿å®ƒæ˜¯é”™è¯¯çš„)ã€‚
å¦‚æœèƒ½è®©ä½ å¥½å—äº›ï¼Œ åœ¨ä½ å¼€å§‹ç²¾ç®€ä¹‹å‰ï¼Œ å¯ä»¥å°†ä½ çš„ä»£ç æ‹·è´åˆ°å¦ä¸€ä¸ªæ–‡ä»¶ä¸­ã€‚
ç„¶åä½ å†æŠŠä¿®æ”¹åçš„ä»£ç ä¸€å—ä¸€å—åœ°æ‹·è´å›å»ã€‚

%ğŸ% Finding a hard bug requires reading, running, ruminating, and
%ğŸ% sometimes retreating.  If you get stuck on one of these activities,
%ğŸ% try the others.

å‘ç°ä¸€ä¸ªé”™è¯¯ï¼Œ éœ€è¦é˜…è¯»ã€è¿è¡Œã€æ²‰æ€ã€å’Œæ—¶è€Œçš„å›é€€ã€‚
å¦‚æœå…¶ä¸­æŸä¸ªæ­¥éª¤æ²¡æœ‰è¿›å±•ï¼Œ è¯•ä¸€ä¸‹å…¶å®ƒçš„ã€‚

%ğŸ% \section{Glossary  |  æœ¯è¯­è¡¨}
\section{æœ¯è¯­è¡¨}
\begin{description}
%ğŸ% \item[deterministic:] Pertaining to a program that does the same
%ğŸ% thing each time it runs, given the same inputs.

\item[ç¡®å®šæ€§çš„ (deterministic)ï¼š] æŒ‡çš„æ˜¯ç»™å®šç›¸åŒçš„è¾“å…¥ï¼Œ ä¸€ä¸ªç¨‹åºæ¯æ¬¡è¿è¡Œçš„ç»“æœæ˜¯ä¸€æ ·çš„ã€‚
\index{deterministic}

%ğŸ% \item[pseudorandom:] Pertaining to a sequence of numbers that appears
%ğŸ% to be random, but is generated by a deterministic program.

\item[ä¼ªéšæœº (pseudorandom)ï¼š] æŒ‡çš„æ˜¯ä¸€ä¸²æ•°å­—çœ‹ä¸Šå»æ˜¯éšæœºçš„ï¼Œ ä½†æ˜¯å®é™…æ˜¯ç”±ä¸€ä¸ªç¡®å®šæ€§ç¨‹åºç”Ÿæˆçš„ã€‚
\index{pseudorandom}

%ğŸ% \item[default value:] The value given to an optional parameter if no
%ğŸ% argument is provided.

\item[é»˜è®¤å€¼ï¼š] æ²¡æœ‰æä¾›å®å‚æ—¶ï¼Œ èµ‹ç»™å¯é€‰å½¢å‚çš„å€¼ã€‚
\index{default value}

%ğŸ% \item[override:] To replace a default value with an argument.

\item[è¦†ç›–ï¼š] ç”¨å®å‚æ›¿ä»£é»˜è®¤å€¼ã€‚
\index{override}

%ğŸ% \item[benchmarking:] The process of choosing between data structures
%ğŸ% by implementing alternatives and testing them on a sample of the
%ğŸ% possible inputs.

\item[åŸºå‡†æµ‹è¯• (benchmarking)ï¼š] é€šè¿‡å¯èƒ½çš„è¾“å…¥æ ·æœ¬å¯¹ä½¿ç”¨ä¸åŒæ•°æ®ç»“æ„çš„å®ç°è¿›è¡Œæµ‹è¯•ï¼Œ ä»è€Œé€‰æ‹©æ•°æ®ç»“æ„çš„è¿‡ç¨‹ã€‚
\index{benchmarking}

%ğŸ% \item[rubber duck debugging:] Debugging by explaining your problem
%ğŸ% to an inanimate object such as a rubber duck.  Articulating the
%ğŸ% problem can help you solve it, even if the rubber duck doesn't know
%ğŸ% Python.

\item[å°é»„é¸­è°ƒè¯•æ³• (rubberducking)ï¼š] é€šè¿‡å‘å°é»„é¸­è¿™æ ·çš„éç”Ÿç‰©ä½“è§£é‡Šä½ çš„é—®é¢˜æ¥è¿›è¡Œè°ƒè¯•ã€‚
    æ¸…æ™°åœ°é™ˆè¿°é—®é¢˜å¯ä»¥å¸®åŠ©ä½ è§£å†³é—®é¢˜ï¼Œ å³ä½¿å°é»„é¸­å¹¶ä¸æ‡‚ Pythonã€‚

\index{rubber duck debugging}
\index{debugging!rubber duck}
\end{description}

%ğŸ% \section{Exercises  |  ç»ƒä¹ }
\section{ç»ƒä¹ }
\begin{exercise}
\index{word frequency}
\index{frequency!word}
\index{Zipf's law}

%ğŸ% The ``rank'' of a word is its position in a list of words
%ğŸ% sorted by frequency: the most common word has rank 1, the
%ğŸ% second most common has rank 2, etc.

å•è¯çš„``ç§©''æ˜¯æŒ‡å®ƒåœ¨æŒ‰ç…§å•è¯é¢‘ç‡æ’åºçš„åˆ—è¡¨ä¸­çš„ä½ç½®ï¼š
å‡ºç°é¢‘ç‡æœ€é«˜çš„å•è¯ï¼Œ å®ƒçš„ç§©æ˜¯{\em 1}ï¼Œ é¢‘ç‡ç¬¬äºŒé«˜çš„å•è¯ï¼Œ å®ƒçš„ç§©æ˜¯{\em 2}ï¼Œ ä»¥æ­¤ç±»æ¨ã€‚

%ğŸ% Zipf's law describes a relationship between the ranks and frequencies
%ğŸ% of words in natural languages
%ğŸ% (\url{http://en.wikipedia.org/wiki/Zipf's_law}).  Specifically, it
%ğŸ% predicts that the frequency, $f$, of the word with rank $r$ is:

\href{http://en.wikipedia.org/wiki/Zipf's_law}{Zipfå®šå¾‹} æè¿°äº†è‡ªç„¶è¯­è¨€ä¸­ç§©å’Œå•è¯å‡ºç°é¢‘ç‡çš„å…³ç³»ã€‚  ç‰¹åˆ«æ˜¯ï¼Œ å®ƒé¢„æµ‹å¯¹äºç§©ä¸º $r$ çš„å•è¯ï¼Œ
å…¶å‡ºç°çš„é¢‘ç‡ $f$ æ˜¯ï¼š

\[ f = c r^{-s} \]

%ğŸ% %
%ğŸ% where $s$ and $c$ are parameters that depend on the language and the
%ğŸ% text.  If you take the logarithm of both sides of this equation, you
%ğŸ% get:

å…¶ä¸­ï¼Œ $s$ å’Œ $c$ æ˜¯ä¾èµ–äºè¯­è¨€å’Œæ–‡æœ¬çš„å‚æ•°ã€‚  å¦‚æœåœ¨ä¸Šè¿°ç­‰å¼ä¸¤è¾¹å–å¯¹æ•°çš„è¯ï¼Œ ä½ å¯ä»¥å¾—åˆ°ï¼š
\index{logarithm}

\[ \log f = \log c - s \log r \]

%ğŸ% %
%ğŸ% So if you plot log $f$ versus log $r$, you should get
%ğŸ% a straight line with slope $-s$ and intercept log $c$.

å› æ­¤ï¼Œ å¦‚æœç»˜å‡º {\em log} $f$ å’Œ {\em log} $r$ çš„å›¾åƒï¼Œ ä½ å¯ä»¥å¾—åˆ°ä¸€æ¡ä»¥ $-s$ã€€ä¸ºæ–œç‡ã€ä»¥ $c$ ä¸ºæˆªè·çš„ç›´çº¿ã€‚

%ğŸ% Write a program that reads a text from a file, counts
%ğŸ% word frequencies, and prints one line
%ğŸ% for each word, in descending order of frequency, with
%ğŸ% log $f$ and log $r$.  Use the graphing program of your
%ğŸ% choice to plot the results and check whether they form
%ğŸ% a straight line.  Can you estimate the value of $s$?

ç¼–å†™ä¸€ä¸ªç¨‹åºï¼Œ ä»æ–‡ä»¶ä¸­è¯»å–æ–‡æœ¬ï¼Œ è®¡ç®—å•è¯é¢‘ç‡ï¼Œ å€’åºè¾“å‡ºæ¯ä¸ªå•è¯ï¼Œ
ä¸€ä¸ªå•è¯ä¸€è¡Œï¼Œ åŒæ—¶åœ¨è¿™è¡Œè¾“å‡ºå¯¹åº”çš„ {\em log} $f$ å’Œ {\em log} $r$ã€‚
ä½¿ç”¨ä½ å–œæ¬¢çš„ç»˜å›¾ç¨‹åºï¼Œ ç”»å‡ºç»“æœå¹¶æ£€æŸ¥æ˜¯ä¸æ˜¯å½¢æˆä¸€æ¡ç›´çº¿ã€‚
ä½ å¯ä»¥ä¼°ç®—å‡º $s$ çš„å€¼å—ï¼Ÿ

%ğŸ% Solution: \url{http://thinkpython2.com/code/zipf.py}.
%ğŸ% To run my solution, you need the plotting module {\tt matplotlib}.
%ğŸ% If you installed Anaconda, you already have {\tt matplotlib};
%ğŸ% otherwise you might have to install it.
\index{matplotlib}

\href{http://thinkpython2.com/code/zipfpy}{å‚è€ƒç­”æ¡ˆ}

ä½ éœ€è¦å®‰è£…ç»˜å›¾æ¨¡å— \href{http://www.matplotlib.org}{{\em \li{matplotlib}}} æ‰èƒ½è¿è¡Œæˆ‘çš„ç­”æ¡ˆã€‚
å½“ç„¶å¦‚æœä½ å®‰è£…äº†  \href{http://www.anaconda.org}{{\em \li{Anaconda}}}ï¼Œ ä½ å°±å·²ç»æœ‰äº† {\em \li{matplotlib}}ï¼›å¦åˆ™ä½ éœ€è¦å®‰è£…å®ƒã€‚
\index{matplotlib}

\end{exercise}