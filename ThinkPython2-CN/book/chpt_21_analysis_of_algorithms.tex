

%ğŸ% \chapter{Analysis of Algorithms  |  ç®—æ³•åˆ†æ}
\chapter{ç®—æ³•åˆ†æ}
\label{algorithms}

%ğŸ% \begin{quote}
%ğŸ% This appendix is an edited excerpt from {\it Think Complexity}, by
%ğŸ% Allen B. Downey, also published by O'Reilly Media (2012).  When you
%ğŸ% are done with this book, you might want to move on to that one.
%ğŸ% \end{quote}

\begin{quote}
æœ¬é™„å½•æ‘˜è‡ª Allen B. Downeyçš„ {\em Think Complexity}ï¼Œ ä¹Ÿç”± Oâ€™Reilly Media (2011)å‡ºç‰ˆã€‚ å½“ä½ è¯»å®Œæœ¬ä¹¦åï¼Œä¹Ÿè®¸ä½ å¯ä»¥æ¥ç€è¯»è¯»é‚£æœ¬ä¹¦ã€‚
\end{quote}

%ğŸ% {\bf Analysis of algorithms} is a branch of computer science that
%ğŸ% studies the performance of algorithms, especially their run time and
%ğŸ% space requirements.  See
%ğŸ% \url{http://en.wikipedia.org/wiki/Analysis_of_algorithms}.

{\em ç®—æ³•åˆ†æ} ({\bf Analysis of algorithms}) æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
 ç€é‡ç ”ç©¶ç®—æ³•çš„æ€§èƒ½ï¼Œ ç‰¹åˆ«æ˜¯ä»–ä»¬çš„è¿è¡Œæ—¶é—´å’Œèµ„æºå¼€é”€ã€‚
è§ï¼š\href{http://en.wikipedia.org/wiki/Analysis_of_algorithms}{ç®—æ³•åˆ†æ\textsuperscript{(ç»´åŸºç™¾ç§‘)}} ã€‚

\index{algorithm} \index{analysis of algorithms}

\index{ç»´åŸºç™¾ç§‘}


%ğŸ% The practical goal of algorithm analysis is to predict the performance
%ğŸ% of different algorithms in order to guide design decisions.

ç®—æ³•åˆ†æçš„å®é™…ç›®çš„æ˜¯é¢„æµ‹ä¸åŒç®—æ³•çš„æ€§èƒ½ï¼Œç”¨äºæŒ‡å¯¼è®¾è®¡å†³ç­–ã€‚

%ğŸ% During the 2008 United States Presidential Campaign, candidate
%ğŸ% Barack Obama was asked to perform an impromptu analysis when
%ğŸ% he visited Google.  Chief executive Eric Schmidt jokingly asked him
%ğŸ% for ``the most efficient way to sort a million 32-bit integers.''
%ğŸ% Obama had apparently been tipped off, because he quickly
%ğŸ% replied, ``I think the bubble sort would be the wrong way to go.''
%ğŸ% See \url{http://www.youtube.com/watch?v=k4RRi_ntQc8}.

2008å¹´ç¾å›½æ€»ç»Ÿå¤§é€‰æœŸé—´ï¼Œå½“å€™é€‰äºº å¥¥å·´é©¬(Barack Obama) è®¿é—® Google æ—¶ï¼Œ
ä»–è¢«è¦æ±‚è¿›è¡Œå³å¸­çš„åˆ†æã€‚  é¦–å¸­æ‰§è¡Œå®˜Eric Schmidtå¼€ç©ç¬‘çš„é—®ä»–
``å¯¹ä¸€ç™¾ä¸‡ä¸ª32ä½æ•´æ•°æ’åºçš„æœ€æœ‰æ•ˆçš„æ–¹æ³•''ã€‚
æ˜¾ç„¶æœ‰äººæš—ä¸­é€šçŸ¥äº†å¥¥å·´é©¬ï¼Œå› ä¸ºä»–å¾ˆå¿«å›ç­”ï¼Œ ``æˆ‘è®¤ä¸ºä¸åº”è¯¥é‡‡ç”¨å†’æ³¡æ’åºæ³•''ã€‚
çœ‹çœ‹\href{http://www.youtube.com/watch?v=k4RRi_ntQc8}{è¿™ä¸ªè§†é¢‘}ã€‚

\index{Obama, Barack}  \index{Schmidt, Eric}  \index{bubble sort}

%ğŸ% This is true: bubble sort is conceptually simple but slow for
%ğŸ% large datasets.  The answer Schmidt was probably looking for is
%ğŸ% ``radix sort'' (\url{http://en.wikipedia.org/wiki/Radix_sort})\footnote{
%ğŸ% But if you get a question like this in an interview, I think
%ğŸ% a better answer is, ``The fastest way to sort a million integers
%ğŸ% is to use whatever sort function is provided by the language
%ğŸ% I'm using.  Its performance is good enough for the vast majority
%ğŸ% of applications, but if it turned out that my application was too
%ğŸ% slow, I would use a profiler to see where the time was being
%ğŸ% spent.  If it looked like a faster sort algorithm would have
%ğŸ% a significant effect on performance, then I would look
%ğŸ% around for a good implementation of radix sort.''}.
\index{radix sort}

æ˜¯çœŸçš„ï¼šå†’æ³¡æ’åºæ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†æ˜¯å¯¹äºå¤§æ•°æ®é›†é€Ÿåº¦éå¸¸æ…¢ã€‚
Schmidt æ‰€æé—®çš„ç­”æ¡ˆå¯èƒ½æ˜¯ ``åŸºæ•°æ’åº (\href{http://en.wikipedia.org/wiki/Radix_sort}{radix sort})''\footnote{ä½†æ˜¯ï¼Œå¦‚æœä½ é‡‡è®¿ä¸­è¢«é—®åˆ°è¿™ä¸ªé—®é¢˜ï¼Œæ›´å¥½çš„ç­”æ¡ˆå¯èƒ½æ˜¯ï¼Œ``å¯¹ä¸Šç™¾ä¸‡ä¸ªæ•´æ•°çš„æœ€å¿«çš„æ’åºæ–¹æ³•å°±æ˜¯ç”¨ä½ æ‰€ä½¿ç”¨çš„è¯­è¨€çš„å†…å»ºæ’åºå‡½æ•°ã€‚ å®ƒçš„æ€§èƒ½å¯¹äºå¤§å¤šæ•°åº”ç”¨è€Œè¨€å·²ä¼˜åŒ–çš„è¶³å¤Ÿå¥½ã€‚ä½†å¦‚æœæ˜¯æˆ‘è‡ªå·±å†™çš„æ’åºç¨‹åºè¿è¡Œå¤ªæ…¢ï¼Œ æˆ‘ä¼šç”¨æ€§èƒ½åˆ†æå™¨æ‰¾å‡ºå¤§é‡çš„è¿ç®—æ—¶é—´è¢«ç”¨åœ¨äº†å“ªå„¿ã€‚å¦‚æœä¸€ä¸ªæ›´å¿«çš„ç®—æ³•ä¼šå¯¹æ€§èƒ½äº§ç”Ÿæ˜¾è‘—çš„æå‡ï¼Œæˆ‘ä¼šå…ˆè¯•è¯•åŸºæ•°æ’åºã€‚''}ã€‚

%ğŸ% The goal of algorithm analysis is to make meaningful
%ğŸ% comparisons between algorithms, but there are some problems:

ç®—æ³•åˆ†æçš„ç›®çš„æ˜¯åœ¨ä¸åŒç®—æ³•é—´è¿›è¡Œæœ‰æ„ä¹‰çš„æ¯”è¾ƒï¼Œ ä½†æ˜¯æœ‰ä¸€äº›é—®é¢˜ï¼š
\index{comparing algorithms}

\begin{itemize}

%ğŸ% \item The relative performance of the algorithms might
%ğŸ% depend on characteristics of the hardware, so one algorithm
%ğŸ% might be faster on Machine A, another on Machine B.
%ğŸ% The general solution to this problem is to specify a
%ğŸ% {\bf machine model} and analyze the number of steps, or
%ğŸ% operations, an algorithm requires under a given model.
%ğŸ% \index{machine model}

\item ç®—æ³•ç›¸å¯¹çš„æ€§èƒ½ä¾èµ–äºç¡¬ä»¶çš„ç‰¹æ€§ï¼Œå› æ­¤ä¸€ä¸ªç®—æ³•å¯èƒ½åœ¨æœºå™¨Aä¸Šæ¯”è¾ƒå¿«ï¼Œ å¦ä¸€ä¸ªç®—æ³•åˆ™åœ¨æœºå™¨Bä¸Šæ¯”è¾ƒå¿«ã€‚
å¯¹æ­¤é—®é¢˜ä¸€èˆ¬çš„è§£å†³åŠæ³•æ˜¯æŒ‡å®šä¸€ä¸ª {\em æœºå™¨æ¨¡å‹} (machine model) å¹¶ä¸”åˆ†æä¸€ä¸ªç®—æ³•åœ¨ä¸€ä¸ªç»™å®šæ¨¡å‹ä¸‹æ‰€éœ€çš„æ­¥éª¤æˆ–è¿ç®—çš„æ•°ç›®ã€‚
\index{machine model} \index{æœºå™¨æ¨¡å‹}

%ğŸ% \item Relative performance might depend on the details of
%ğŸ% the dataset.  For example, some sorting
%ğŸ% algorithms run faster if the data are already partially sorted;
%ğŸ% other algorithms run slower in this case.
%ğŸ% A common way to avoid this problem is to analyze the
%ğŸ% {\bf worst case} scenario.  It is sometimes useful to
%ğŸ% analyze average case performance, but that's usually harder,
%ğŸ% and it might not be obvious what set of cases to average over.
%ğŸ% \index{worst case}  \index{average case}

\item ç›¸å¯¹æ€§èƒ½å¯èƒ½ä¾èµ–äºæ•°æ®é›†çš„ç»†èŠ‚ã€‚
ä¾‹å¦‚ï¼Œ å¦‚æœæ•°æ®å·²ç»éƒ¨åˆ†æ’å¥½åºï¼Œ ä¸€äº›æ’åºç®—æ³•å¯èƒ½æ›´å¿«ï¼› æ­¤æ—¶å…¶å®ƒç®—æ³•è¿è¡Œçš„æ¯”è¾ƒæ…¢ã€‚
é¿å…è¯¥é—®é¢˜çš„ä¸€èˆ¬æ–¹æ³•æ˜¯åˆ†æ {\em æœ€åæƒ…å†µ}ã€‚
æœ‰æ—¶åˆ†æå¹³å‡æƒ…å†µæ€§èƒ½ï¼Œ ä½†é‚£é€šå¸¸æ›´éš¾, è€Œä¸”å¯èƒ½ä¸å®¹æ˜“å¼„æ¸…è¯¥å¯¹å“ªäº›æ•°æ®é›†åˆè¿›è¡Œå¹³å‡ã€‚
\index{worst case}  \index{average case}

%ğŸ% \item Relative performance also depends on the size of the
%ğŸ% problem.  A sorting algorithm that is fast for small lists
%ğŸ% might be slow for long lists.
%ğŸ% The usual solution to this problem is to express run time
%ğŸ% (or number of operations) as a function of problem size,
%ğŸ% and group functions into categories depending on how quickly
%ğŸ% they grow as problem size increases.

\item ç›¸å¯¹æ€§èƒ½ä¹Ÿä¾èµ–äºé—®é¢˜çš„è§„æ¨¡ã€‚
ä¸€ä¸ªå¯¹äºå°åˆ—è¡¨å¾ˆå¿«çš„æ’åºç®—æ³•å¯èƒ½å¯¹äºé•¿åˆ—è¡¨å¾ˆæ…¢ã€‚
å¯¹æ­¤é—®é¢˜é€šå¸¸çš„è§£å†³æ–¹æ³•æ˜¯å°†è¿è¡Œæ—¶é—´ï¼ˆæˆ–è€…è¿ç®—çš„æ•°ç›®ï¼‰è¡¨ç¤ºæˆé—®é¢˜è§„æ¨¡çš„å‡½æ•°ï¼Œ å¹¶ä¸”æ ¹æ®å„è‡ªéšç€é—®é¢˜è§„æ¨¡çš„å¢é•¿è€Œå¢åŠ çš„é€Ÿåº¦ï¼Œå°†å‡½æ•°åˆ†æˆä¸åŒçš„ç±»åˆ«ã€‚
\end{itemize}

%ğŸ% The good thing about this kind of comparison is that it lends
%ğŸ% itself to simple classification of algorithms.  For example,
%ğŸ% if I know that the run time of Algorithm A tends to be
%ğŸ% proportional to the size of the input, $n$, and Algorithm B
%ğŸ% tends to be proportional to $n^2$, then I
%ğŸ% expect A to be faster than B, at least for large values of $n$.

å…³äºæ­¤ç±»æ¯”è¾ƒçš„å¥½å¤„æ˜¯æœ‰åŠ©äºå¯¹ç®—æ³•è¿›è¡Œç®€å•çš„åˆ†ç±»ã€‚
ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘çŸ¥é“ç®—æ³• A çš„è¿è¡Œæ—¶é—´ä¸è¾“å…¥çš„è§„æ¨¡ $n$ æˆæ­£æ¯”ï¼Œ
ç®—æ³• B ä¸ $n^2$ æˆæ­£æ¯”ï¼Œé‚£ä¹ˆæˆ‘ å¯ä»¥è®¤ä¸ºA æ¯” B å¿«ï¼Œ è‡³å°‘å¯¹äºå¾ˆå¤§çš„ $n$ å€¼æ¥è¯´æ˜¯è¿™æ ·ã€‚

%ğŸ% This kind of analysis comes with some caveats, but we'll get
%ğŸ% to that later.

è¿™ç±»åˆ†æä¹Ÿæœ‰ä¸€äº›é—®é¢˜ï¼Œåé¢ä¼šä½œæåŠã€‚


%ğŸ% \section{Order of growth  |  å¢é•¿é‡çº§}
\section{å¢é•¿é‡çº§}

%ğŸ% Suppose you have analyzed two algorithms and expressed
%ğŸ% their run times in terms of the size of the input:
%ğŸ% Algorithm A takes $100n+1$ steps to solve a problem with
%ğŸ% size $n$; Algorithm B takes $n^2 + n + 1$ steps.

å‡è®¾ä½ å·²ç»åˆ†æäº†ä¸¤ä¸ªç®—æ³•ï¼Œå¹¶èƒ½ç”¨è¾“å…¥è®¡ç®—é‡çš„è§„æ¨¡è¡¨ç¤ºå®ƒä»¬çš„è¿è¡Œæ—¶é—´ï¼š
è‹¥ç®—æ³• A ç”¨ $100n+1$ æ­¥è§£å†³ä¸€ä¸ªè§„æ¨¡ä¸º $n$ çš„é—®é¢˜ï¼›
è€Œç®—æ³• B ç”¨ $n^2 + n + 1$ æ­¥ã€‚
\index{order of growth}  \index{å¢é•¿çš„é˜¶æ•°}

%ğŸ% The following table shows the run time of these algorithms
%ğŸ% for different problem sizes:

ä¸‹è¡¨åˆ—å‡ºäº†è¿™äº›ç®—æ³•å¯¹äºä¸åŒé—®é¢˜è§„æ¨¡çš„è¿è¡Œæ—¶é—´ï¼š

\begin{center}
\begin{tabular}{|r|r|r|}
\hline
Input     &   Run time of     & Run time of \\
size      &   Algorithm A     & Algorithm B \\
\hline
10        &   1 001           & 111         \\
100       &   10 001          & 10 101         \\
1 000     &   100 001         & 1 001 001         \\
10 000    &   1 000 001       & $> 10^{10}$         \\
\hline
\end{tabular}
\end{center}

%ğŸ% At $n=10$, Algorithm A looks pretty bad; it takes almost 10 times
%ğŸ% longer than Algorithm B.  But for $n=100$ they are about the same, and
%ğŸ% for larger values A is much better.

å½“ $n=10$ æ—¶ï¼Œç®—æ³• A çœ‹ä¸Šå»å¾ˆç³Ÿç³•ï¼Œå®ƒç”¨äº† 10 å€äºç®—æ³• B æ‰€éœ€çš„æ—¶é—´ã€‚
ä½†å½“ $n=100$ æ—¶ ï¼Œå®ƒä»¬æ€§èƒ½å‡ ä¹ç›¸åŒï¼Œ è€Œ $n$ å–æ›´å¤§å€¼æ—¶ï¼Œç®—æ³• A è¦å¥½å¾—å¤šã€‚

%ğŸ% The fundamental reason is that for large values of $n$, any function
%ğŸ% that contains an $n^2$ term will grow faster than a function whose
%ğŸ% leading term is $n$.  The {\bf leading term} is the term with the
%ğŸ% highest exponent.

æ ¹æœ¬åŸå› æ˜¯å¯¹äºè¾ƒå¤§çš„ $n$ å€¼ï¼Œä»»ä½•åŒ…å« $n^2$ é¡¹çš„å‡½æ•°éƒ½æ¯”é¦–é¡¹ä¸º $n$ çš„å‡½æ•°å¢é•¿è¦å¿«ã€‚
{\em é¦–é¡¹} (leading term) æ˜¯æŒ‡å…·æœ‰æœ€é«˜æŒ‡æ•°çš„é¡¹ã€‚
\index{leading term}  \index{exponent}

%ğŸ% For Algorithm A, the leading term has a large coefficient, 100, which
%ğŸ% is why B does better than A for small $n$.  But regardless of the
%ğŸ% coefficients, there will always be some value of $n$ where
%ğŸ% $a n^2 > b n$, for any values of $a$ and $b$.

å¯¹äºç®—æ³•Aï¼Œé¦–é¡¹æœ‰ä¸€ä¸ªè¾ƒå¤§çš„ç³»æ•° 100ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¯¹äºå° $n$ ï¼ŒB æ¯” A å¥½ã€‚
ä½†æ˜¯ä¸è€ƒè™‘è¯¥ç³»æ•°ï¼Œæ€»æœ‰ä¸€äº› $n$ å€¼ä½¿å¾— $a n^2 > b n$ã€‚
\index{leading coefficient}

%ğŸ% The same argument applies to the non-leading terms.  Even if the run
%ğŸ% time of Algorithm A were $n+1000000$, it would still be better than
%ğŸ% Algorithm B for sufficiently large $n$.

åŒæ ·çš„ç†ç”±é€‚ç”¨äºéé¦–é¡¹ã€‚
å³ä½¿ç®—æ³• A çš„è¿è¡Œæ—¶é—´ä¸º $n+1000000$ ï¼Œå¯¹äºè¶³å¤Ÿå¤§çš„ $n$ ï¼Œå®ƒä»ç„¶æ¯”ç®—æ³• B å¥½ã€‚

%ğŸ% In general, we expect an algorithm with a smaller leading term to be a
%ğŸ% better algorithm for large problems, but for smaller problems, there
%ğŸ% may be a {\bf crossover point} where another algorithm is better.  The
%ğŸ% location of the crossover point depends on the details of the
%ğŸ% algorithms, the inputs, and the hardware, so it is usually ignored for
%ğŸ% purposes of algorithmic analysis.  But that doesn't mean you can forget
%ğŸ% about it.

ä¸€èˆ¬æ¥è®²ï¼Œæˆ‘ä»¬è®¤ä¸ºå…·å¤‡è¾ƒå°é¦–é¡¹çš„ç®—æ³•å¯¹äºè§„æ¨¡å¤§çš„é—®é¢˜æ˜¯ä¸€ä¸ªå¥½ç®—æ³•ï¼Œä½†æ˜¯å¯¹äºè§„æ¨¡å°çš„é—®é¢˜ï¼Œå¯èƒ½å­˜åœ¨ä¸€ä¸ª {\em äº¤å‰ç‚¹} (crossover point) ï¼Œ åœ¨æ­¤è§„æ¨¡ä»¥ä¸‹ï¼Œå¦ä¸€ä¸ªç®—æ³•æ›´å¥½ã€‚  äº¤å‰ç‚¹çš„ä½ç½®å–å†³äºç®—æ³•çš„ç»†èŠ‚ã€ è¾“å…¥ä»¥åŠç¡¬ä»¶ï¼Œå› æ­¤å¯¹äºç®—æ³•åˆ†æç›®çš„ï¼Œå®ƒé€šå¸¸è¢«å¿½ç•¥ã€‚
ä½†æ˜¯è¿™ä¸æ„å‘³ç€ä½ å¯ä»¥å¿˜è®°å®ƒçš„å­˜åœ¨ã€‚
\index{crossover point}

%ğŸ% If two algorithms have the same leading order term, it is hard to say
%ğŸ% which is better; again, the answer depends on the details.  So for
%ğŸ% algorithmic analysis, functions with the same leading term
%ğŸ% are considered equivalent, even if they have different coefficients.

å¦‚æœä¸¤ä¸ªç®—æ³•æœ‰ç›¸åŒçš„é¦–é¡¹ï¼Œå¾ˆéš¾è¯´å“ªä¸ªæ›´å¥½ã€‚ ç­”æ¡ˆè¿˜æ˜¯å–å†³äºç»†èŠ‚ã€‚
æ‰€ä»¥ï¼Œå¯¹äºç®—æ³•åˆ†ææ¥è¯´ï¼Œå…·æœ‰ç›¸åŒé¦–é¡¹çš„å‡½æ•°è¢«è®¤ä¸ºæ˜¯ç›¸å½“çš„ï¼Œå³ä½¿å®ƒä»¬å…·æœ‰ä¸åŒçš„ç³»æ•°ã€‚

%ğŸ% An {\bf order of growth} is a set of functions whose growth
%ğŸ% behavior is considered equivalent.  For example, $2n$, $100n$ and $n+1$
%ğŸ% belong to the same order of growth, which is written $O(n)$ in
%ğŸ% {\bf Big-Oh notation} and often called {\bf linear} because every function
%ğŸ% in the set grows linearly with $n$.

{\em å¢é•¿é‡çº§}\footnote{è¯‘æ³¨ï¼šåˆè¯‘{\em å¢é•¿çº§}ï¼Œ{\em å¢é•¿é˜¶æ•°}ï¼› å³ï¼šç®—æ³•æ€§èƒ½} (order of growth) æ˜¯ä¸€ä¸ªå‡½æ•°é›†åˆï¼Œ é›†åˆä¸­å‡½æ•°çš„å¢é•¿è¡Œä¸ºè¢«è®¤ä¸ºæ˜¯ç›¸å½“çš„ã€‚
ä¾‹å¦‚ $2n$ ã€ $100n$ å’Œ $n+1$ å±äºç›¸åŒçš„å¢é•¿é‡çº§ï¼Œ
ç”¨ \href{https://zh.wikipedia.org/wiki/%E5%A4%A7O%E7%AC%A6%E5%8F%B7}{{\em å¤§{\em O}ç¬¦å·}} (Big-Oh notation)è¡¨ç¤ºï¼Œ å†™æˆ $O(n)$ ï¼Œ
è€Œä¸”é€šå¸¸è¢«ç§°ä½œ {\em çº¿æ€§çš„} (linear) ï¼Œ
å› ä¸ºé›†åˆä¸­çš„æ¯ä¸ªå‡½æ•°æ ¹æ® $n$ çº¿æ€§å¢é•¿ã€‚
\index{big-oh notation}  \index{linear growth}

%ğŸ% All functions with the leading term $n^2$ belong to $O(n^2)$; they are
%ğŸ% called {\bf quadratic}.

é¦–é¡¹ä¸º $n^2$ çš„å‡½æ•°å±äº $O(n^2)$ï¼›  å®ƒä»¬æ˜¯ {\em äºŒæ¬¡æ–¹çº§} (quadratic) ï¼Œ

\index{quadratic growth}

%ğŸ% The following table shows some of the orders of growth that
%ğŸ% appear most commonly in algorithmic analysis,
%ğŸ% in increasing order of badness.

ä¸‹è¡¨æŒ‰ç…§è®¡ç®—æ€§èƒ½å¼€é”€æ•ˆç‡é™åºé¡ºåºæ’åˆ—æ˜¾ç¤ºäº†ç®—æ³•åˆ†æä¸­æœ€é€šå¸¸çš„ä¸€äº›å¢é•¿é‡çº§ã€‚
\index{badness}

%ğŸ%  \begin{tabular}{|r|r|r|}
%ğŸ%  \hline
%ğŸ%  Order of     &   Name      \\
%ğŸ%  growth       &               \\
%ğŸ%  \hline
%ğŸ%  $O(1)$             & constant \\
%ğŸ%  $O(\log_b n)$      & logarithmic (for any $b$) \\
%ğŸ%  $O(n)$             & linear \\
%ğŸ%  $O(n \log_b n)$    & linearithmic \\
%ğŸ%  $O(n^2)$           & quadratic     \\
%ğŸ%  $O(n^3)$           & cubic     \\
%ğŸ%  $O(c^n)$           & exponential (for any $c$)    \\
%ğŸ%  \hline
%ğŸ%  \end{tabular}

\begin{center}
\begin{tabular}{|r|r|r|}
\hline
å¢é•¿é‡çº§     &   åç§°      \\
\hline
$O(1)$             & å¸¸æ•°çº§ (constant) \\
$O(\log_b n)$      & å¯¹æ•°çº§ (logarithmic) (å¯¹äºä»»æ„ $b$) \\
$O(n)$             & çº¿æ€§çº§ (linear)\\
$O(n \log_b n)$    & çº¿æ€§å¯¹æ•°çº§ (linearithmic)\\
$O(n^2)$           & äºŒæ¬¡æ–¹çº§ (quadratic)    \\
$O(n^3)$           & ä¸‰æ¬¡æ–¹çº§ (cubic)    \\
$O(c^n)$           & æŒ‡æ•°çº§ (exponential) (å¯¹äºä»»æ„ $c$)    \\
\hline
\end{tabular}
\end{center}

%ğŸ% For the logarithmic terms, the base of the logarithm doesn't matter;
%ğŸ% changing bases is the equivalent of multiplying by a constant, which
%ğŸ% doesn't change the order of growth.  Similarly, all exponential
%ğŸ% functions belong to the same order of growth regardless of the base of
%ğŸ% the exponent. Exponential functions grow very quickly,
%ğŸ% so exponential algorithms are only useful for small problems.

å¯¹äºå¯¹æ•°çº§æ•°ï¼Œ å¯¹æ•°çš„åŸºæ•°å¹¶ä¸å½±å“å¢é•¿é‡çº§ã€‚
æ”¹å˜é˜¶æ•°ç­‰ä»·äºä¹˜ä»¥ä¸€ä¸ªå¸¸æ•°ï¼Œ å…¶ä¸æ”¹å˜å¢é•¿é‡çº§ã€‚
ç›¸åº”çš„ï¼Œ æ‰€æœ‰çš„æŒ‡æ•°çº§æ•°éƒ½å±äºç›¸åŒçš„å¢é•¿é‡çº§ï¼Œ è€Œæ— éœ€è€ƒè™‘æŒ‡æ•°çš„åŸºæ•°å¤§å°
æŒ‡æ•°å‡½æ•°å¢é•¿é‡çº§å¢é•¿çš„éå¸¸å¿«ï¼Œ å› æ­¤æŒ‡æ•°çº§ç®—æ³•åªç”¨äºå°è§„æ¨¡é—®é¢˜ã€‚
\index{logarithmic growth}  \index{exponential growth}


\begin{exercise}

%ğŸ% Read the Wikipedia page on Big-Oh notation at
%ğŸ% \url{http://en.wikipedia.org/wiki/Big_O_notation} and
%ğŸ% answer the following questions:

é˜…è¯»ç»´åŸºç™¾ç§‘å…³äº \href{http://en.wikipedia.org/wiki/Big_O_notation}{å¤§Oç¬¦å·} çš„ä»‹ç»ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

\index{ç»´åŸºç™¾ç§‘}

\begin{enumerate}
%ğŸ% \item What is the order of growth of $n^3 + n^2$?
%ğŸ% What about $1000000 n^3 + n^2$?
%ğŸ% What about $n^3 + 1000000 n^2$?

\item $n^3 + n^2$ çš„å¢é•¿é‡çº§æ˜¯å¤šå°‘ï¼Ÿ  $1000000 n^3 + n^2$ å’Œ $n^3 + 1000000 n^2$ çš„å¢é•¿é‡çº§åˆæ˜¯å¤šå°‘ï¼Ÿ

%ğŸ% \item What is the order of growth of $(n^2 + n) \cdot (n + 1)$?  Before
%ğŸ%   you start multiplying, remember that you only need the leading term.

\item $(n^2 + n) \cdot (n + 1)$ çš„å¢é•¿é‡çº§æ˜¯å¤šå°‘ï¼Ÿ  åœ¨å¼€å§‹è®¡ç®—ä¹‹å‰ï¼Œè®°ä½ä½ åªéœ€è¦è€ƒè™‘é¦–é¡¹å³å¯ã€‚

%ğŸ% \item If $f$ is in $O(g)$, for some unspecified function $g$, what can
%ğŸ%   we say about $af+b$?

\item å¦‚æœ $f$ çš„å¢é•¿é‡çº§ä¸º $O(g)$ ï¼Œé‚£ä¹ˆå¯¹äºæœªæŒ‡å®šçš„å‡½æ•° $g$ ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä½•æè¿° $af+b$ ï¼Ÿ

%ğŸ% \item If $f_1$ and $f_2$ are in $O(g)$, what can we say about $f_1 + f_2$?

\item å¦‚æœ $f_1$ å’Œ $f_2$ çš„å¢é•¿é‡çº§ä¸º $O(g)$ï¼Œé‚£ä¹ˆ $f_1 + f_2$ çš„å¢é•¿é‡çº§åˆæ˜¯å¤šå°‘ï¼Ÿ

%ğŸ% \item If  $f_1$ is in $O(g)$
%ğŸ% and $f_2$ is in $O(h)$,
%ğŸ% what can we say about  $f_1 + f_2$?

\item å¦‚æœ $f_1$ çš„å¢é•¿é‡çº§ä¸º $O(g)$ ï¼Œ$f_2$ çš„å¢é•¿é‡çº§ä¸º $O(h)$ï¼Œé‚£ä¹ˆ $f_1 + f_2$ çš„å¢é•¿é‡çº§æ˜¯å¤šå°‘ï¼Ÿ

%ğŸ% \item If  $f_1$ is in $O(g)$ and $f_2$ is $O(h)$,
%ğŸ% what can we say about  $f_1 \cdot f_2$?
\end{enumerate}

\end{exercise}

%ğŸ% Programmers who care about performance often find this kind of
%ğŸ% analysis hard to swallow.  They have a point: sometimes the
%ğŸ% coefficients and the non-leading terms make a real difference.
%ğŸ% Sometimes the details of the hardware, the programming language, and
%ğŸ% the characteristics of the input make a big difference.  And for small
%ğŸ% problems asymptotic behavior is irrelevant.

å…³æ³¨æ€§èƒ½çš„ç¨‹åºå‘˜ç»å¸¸å‘ç°è¿™ç§åˆ†æå¾ˆéš¾å¿å—ã€‚  ä»–ä»¬æœ‰ä¸€ä¸ªè§‚ç‚¹ï¼šæœ‰æ—¶ç³»æ•°å’Œéé¦–é¡¹ä¼šé€ æˆå·¨å¤§çš„å½±å“ã€‚  æœ‰æ—¶ï¼Œç¡¬ä»¶çš„ç»†èŠ‚ã€ç¼–ç¨‹è¯­è¨€ä»¥åŠè¾“å…¥çš„ç‰¹æ€§ä¼šé€ æˆå¾ˆå¤§çš„å½±å“ã€‚  å¯¹äºå°é—®é¢˜ï¼Œæ¸è¿‘çš„è¡Œä¸ºæ²¡æœ‰ä»€ä¹ˆå½±å“ã€‚

%ğŸ% But if you keep those caveats in mind, algorithmic analysis is a
%ğŸ% useful tool.  At least for large problems, the ``better'' algorithms
%ğŸ% is usually better, and sometimes it is {\em much} better.  The
%ğŸ% difference between two algorithms with the same order of growth is
%ğŸ% usually a constant factor, but the difference between a good algorithm
%ğŸ% and a bad algorithm is unbounded!

ä½†æ˜¯ï¼Œå¦‚æœä½ è®°å¾—é‚£äº›è­¦å‘Šï¼Œç®—æ³•åˆ†æå°±æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„å·¥å…·ã€‚
è‡³å°‘å¯¹äºå¤§é—®é¢˜ ``æ›´å¥½çš„'' ç®—æ³•é€šå¸¸æ›´å¥½ï¼Œå¹¶ä¸”æœ‰æ—¶å®ƒè¦å¥½çš„å¤šã€‚
ç›¸åŒå¢é•¿é‡çº§çš„ä¸¤ä¸ªç®—æ³•ä¹‹é—´çš„ä¸åŒé€šå¸¸æ˜¯ä¸€ä¸ªå¸¸æ•°å› å­ï¼Œ
ä½†æ˜¯ä¸€ä¸ªå¥½ç®—æ³•å’Œä¸€ä¸ªåç®—æ³•ä¹‹é—´çš„ä¸åŒæ˜¯æ— é™çš„ï¼


%ğŸ% \section{Analysis of basic Python operations  |  PythonåŸºæœ¬è¿ç®—æ“ä½œåˆ†æ}
\section{PythonåŸºæœ¬è¿ç®—æ“ä½œåˆ†æ}

%ğŸ% In Python, most arithmetic operations are constant time;
%ğŸ% multiplication usually takes longer than addition and subtraction, and
%ğŸ% division takes even longer, but these run times don't depend on the
%ğŸ% magnitude of the operands.  Very large integers are an exception; in
%ğŸ% that case the run time increases with the number of digits.

åœ¨ Python ä¸­ï¼Œå¤§éƒ¨åˆ†ç®—æœ¯è¿ç®—çš„å¼€é”€æ˜¯å¸¸æ•°çº§çš„ã€‚
ä¹˜æ³•ä¼šæ¯”åŠ å‡æ³•ç”¨æ›´é•¿çš„æ—¶é—´ï¼Œé™¤æ³•æ›´é•¿ï¼Œä½†æ˜¯è¿™äº›è¿ç®—æ—¶é—´ä¸ä¾èµ–è¢«è¿ç®—æ•°çš„æ•°é‡çº§ã€‚
éå¸¸å¤§çš„æ•´æ•°å´æ˜¯ä¸ªä¾‹å¤–ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿è¡Œæ—¶é—´éšç€ä½æ•°çš„å¢åŠ è€Œå¢åŠ ã€‚
\index{analysis of primitives}

%ğŸ% Indexing operations---reading or writing elements in a sequence
%ğŸ% or dictionary---are also constant time, regardless of the size
%ğŸ% of the data structure.

ç´¢å¼•æ“ä½œ --- åœ¨åºåˆ—æˆ–å­—å…¸ä¸­è¯»å†™å…ƒç´  --- åœ¨ä¸è€ƒè™‘æ•°æ®ç»“æ„çš„å¤§å°çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¸¸æ•°çº§çš„ã€‚
\index{indexing}

%ğŸ% A {\tt for} loop that traverses a sequence or dictionary is
%ğŸ% usually linear, as long as all of the operations in the body
%ğŸ% of the loop are constant time.  For example, adding up the
%ğŸ% elements of a list is linear:

ä¸€ä¸ªéå†åºåˆ—æˆ–å­—å…¸çš„ \li{for} å¾ªç¯é€šå¸¸æ˜¯çº¿æ€§çš„ï¼Œ åªè¦å¾ªç¯ä½“å†…çš„è¿ç®—æ˜¯å¸¸æ•°æ—¶é—´ã€‚
ä¾‹å¦‚ï¼Œç´¯åŠ ä¸€ä¸ªåˆ—è¡¨çš„å…ƒç´ æ˜¯çº¿æ€§çš„ï¼š

\begin{lstlisting}
    total = 0
    for x in t:
        total += x
\end{lstlisting}

%ğŸ% The built-in function {\tt sum} is also linear because it does
%ğŸ% the same thing, but it tends to be faster because it is a more
%ğŸ% efficient implementation; in the language of algorithmic analysis,
%ğŸ% it has a smaller leading coefficient.

å†…å»ºæ±‚å’Œå‡½æ•° \li{sum} ä¹Ÿæ˜¯çº¿æ€§çš„ï¼Œå› ä¸ºå®ƒåšç›¸åŒçš„äº‹æƒ…ï¼Œ
ä½†æ˜¯å®ƒå€¾å‘äºæ›´å¿«å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªæ›´æœ‰æ•ˆçš„å®ç°ã€‚
ä»ç®—æ³•åˆ†æè§’åº¦è®²ï¼Œå®ƒå…·æœ‰æ›´å°çš„é¦–é¡¹ç³»æ•°ã€‚

%ğŸ% As a rule of thumb, if the body of a loop is in $O(n^a)$ then
%ğŸ% the whole loop is in $O(n^{a+1})$.  The exception is if you can
%ğŸ% show that the loop exits after a constant number of iterations.
%ğŸ% If a loop runs $k$ times regardless of $n$, then
%ğŸ% the loop is in $O(n^a)$, even for large $k$.

æ ¹æ®ç»éªŒ, å¦‚æœå¾ªç¯ä½“å†…çš„å¢é•¿é‡çº§æ˜¯ $O(n^a)$, åˆ™æ•´ä¸ªå¾ªç¯çš„å¢é•¿é‡çº§æ˜¯ $O(n^{a+1})$ã€‚
å¦‚æœè¿™ä¸ªå¾ªç¯åœ¨æ‰§è¡Œä¸€å®šæ•°ç›®å¾ªç¯åé€€å‡ºåˆ™æ˜¯ä¾‹å¤–ã€‚ æ— è®º $n$ å–å€¼å¤šå°‘ï¼Œ å¦‚æœå¾ªç¯ä»…æ‰§è¡Œ $k$ æ¬¡ï¼Œ æ•´ä¸ªå¾ªç¯çš„å¢é•¿é‡çº§æ˜¯ $O(n^a)$ï¼Œ å³ä¾¿ $k$ å€¼æ¯”è¾ƒå¤§ã€‚

%ğŸ% Multiplying by $k$ doesn't change the order of growth, but neither
%ğŸ% does dividing.  So if the body of a loop is in $O(n^a)$ and it runs
%ğŸ% $n/k$ times, the loop is in $O(n^{a+1})$, even for large $k$.

ä¹˜ä¸Š $k$ å¹¶ä¸ä¼šæ”¹å˜å¢é•¿é‡çº§ï¼Œé™¤æ³•ä¹Ÿæ˜¯ã€‚ å› æ­¤ï¼Œå¦‚æœå¾ªç¯ä½“çš„å¢é•¿é‡çº§æ˜¯ $O(n^a)$ï¼Œ å¾ªç¯æ‰§è¡Œ $n/k$ æ¬¡ï¼Œ é‚£ä¹ˆæ•´ä¸ªå¾ªç¯çš„å¢é•¿é‡çº§å°±æ˜¯ $O(n^{a+1})$ , å³ä½¿ $k$ å€¼å¾ˆå¤§ã€‚

%ğŸ% Most string and tuple operations are linear, except indexing and {\tt
%ğŸ%   len}, which are constant time.  The built-in functions {\tt min} and
%ğŸ% {\tt max} are linear.  The run-time of a slice operation is
%ğŸ% proportional to the length of the output, but independent of the size
%ğŸ% of the input.

å¤§éƒ¨åˆ†å­—ç¬¦ä¸²å’Œå…ƒç»„è¿ç®—æ˜¯çº¿æ€§çš„ï¼Œé™¤äº†ç´¢å¼•å’Œ \li{len}ï¼Œå®ƒä»¬æ˜¯å¸¸æ•°æ—¶é—´ã€‚
å†…å»ºå‡½æ•° \li{min} å’Œ \li{max} æ˜¯çº¿æ€§çš„ã€‚
åˆ‡ç‰‡è¿ç®—ä¸è¾“å‡ºçš„é•¿åº¦æˆæ­£æ¯”ï¼Œä½†æ˜¯å’Œè¾“å…¥çš„å¤§å°æ— å…³ã€‚
\index{string methods}  \index{tuple methods}
\index{å­—ç¬¦ä¸²æ–¹æ³•}  \index{å…ƒç»„æ–¹æ³•}

%ğŸ% String concatenation is linear; the run time depends on the sum
%ğŸ% of the lengths of the operands.

å­—ç¬¦ä¸²æ‹¼æ¥æ˜¯çº¿æ€§çš„ï¼› å®ƒçš„è¿ç®—æ—¶é—´å–å†³äº è¿ç®—å¯¹è±¡ çš„æ€»é•¿åº¦ã€‚
\index{string concatenation}

%ğŸ% All string methods are linear, but if the lengths of
%ğŸ% the strings are bounded by a constant---for example, operations on single
%ğŸ% characters---they are considered constant time.
%ğŸ% The string method {\tt join} is linear; the run time depends on
%ğŸ% the total length of the strings.

æ‰€æœ‰å­—ç¬¦ä¸²æ–¹æ³•æ˜¯çº¿æ€§çš„ï¼Œä½†æ˜¯å¦‚æœå­—ç¬¦ä¸²çš„é•¿åº¦å—é™äºä¸€ä¸ªå¸¸æ•° --- ä¾‹å¦‚ï¼Œ
åœ¨å•ä¸ªå­—ç¬¦ä¸Šçš„è¿ç®— --- å®ƒä»¬è¢«è®¤ä¸ºæ˜¯å¸¸æ•°æ—¶é—´ã€‚
å­—ç¬¦ä¸²æ–¹æ³• \li{join} ä¹Ÿæ˜¯çº¿æ€§çš„ï¼› å®ƒçš„è¿ç®—æ—¶é—´å–å†³äºå­—ç¬¦ä¸²çš„æ€»é•¿åº¦ã€‚

\index{join@{\tt join}}

%ğŸ% Most list methods are linear, but there are some exceptions:

å¤§éƒ¨åˆ†åˆ—è¡¨æ–¹æ³•æ˜¯çº¿æ€§çš„ï¼Œä½†æ˜¯æœ‰ä¸€äº›ä¾‹å¤–ï¼š
\index{list methods}

\begin{itemize}

%ğŸ% \item Adding an element to the end of a list is constant time on
%ğŸ% average; when it runs out of room it occasionally gets copied
%ğŸ% to a bigger location, but the total time for $n$ operations
%ğŸ% is $O(n)$, so the average time for each
%ğŸ% operation is $O(1)$.

\item å¹³å‡æ¥è®²ï¼Œåœ¨åˆ—è¡¨ç»“å°¾å¢åŠ ä¸€ä¸ªå…ƒç´ æ˜¯å¸¸æ•°æ—¶é—´ã€‚ å½“å®ƒè¶…å‡ºäº†æ‰€å ç”¨ç©ºé—´æ—¶ï¼Œå®ƒå¶å°”è¢«æ‹·è´åˆ°ä¸€ä¸ªæ›´å¤§çš„åœ°æ–¹ï¼Œ ä½†æ˜¯å¯¹äº $n$ ä¸ªè¿ç®—çš„æ•´ä½“æ—¶é—´ä»ä¸º $O(n)$ ï¼Œ æ‰€ä»¥æˆ‘ä»¬è¯´ä¸€ä¸ªè¿ç®—çš„â€œåˆ†æ‘Šâ€æ—¶é—´æ˜¯ $O(1)$ ã€‚

%ğŸ% \item Removing an element from the end of a list is constant time.

\item ä»ä¸€ä¸ªåˆ—è¡¨ç»“å°¾åˆ é™¤ä¸€ä¸ªå…ƒç´ æ˜¯å¸¸æ•°æ—¶é—´ã€‚

%ğŸ% \item Sorting is $O(n \log n)$.

\item æ’åºæ˜¯ $O(n \log n)$ ã€‚
\index{sorting}  \index{æ’åº}

\end{itemize}

%ğŸ% Most dictionary operations and methods are constant time, but
%ğŸ% there are some exceptions:

å¤§éƒ¨åˆ†å­—å…¸è¿ç®—å’Œæ–¹æ³•æ˜¯å¸¸æ•°æ—¶é—´ï¼Œä½†æœ‰äº›ä¾‹å¤–ï¼š
\index{dictionary methods}

\begin{itemize}

%ğŸ% \item The run time of {\tt update} is
%ğŸ%   proportional to the size of the dictionary passed as a parameter,
%ğŸ%   not the dictionary being updated.

\item \li{update} çš„è¿è¡Œæ—¶é—´ä¸ä½œä¸ºå½¢å‚è¢«ä¼ é€’çš„å­—å…¸ï¼ˆä¸æ˜¯è¢«æ›´æ–°çš„å­—å…¸ï¼‰çš„å¤§å°æˆæ­£æ¯”ã€‚

%ğŸ% \item {\tt keys}, {\tt values} and {\tt items} are constant time because
%ğŸ%   they return iterators.  But if you loop through the iterators, the loop will be linear.

\item \li{keys}ã€ \li{values} å’Œ \li{items} æ˜¯å¸¸æ•°æ—¶é—´ï¼Œå› ä¸ºå®ƒä»¬è¿”å›è¿­ä»£å™¨ã€‚
   ä½†æ˜¯å¦‚æœä½ å¯¹è¿­ä»£å™¨è¿›è¡Œå¾ªç¯ï¼Œå¾ªç¯å°†æ˜¯çº¿æ€§çš„ã€‚
\index{iterator}

\end{itemize}

%ğŸ% The performance of dictionaries is one of the minor miracles of
%ğŸ% computer science.  We will see how they work in
%ğŸ% Section~\ref{hashtable}.

å­—å…¸çš„æ€§èƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªå°å¥‡è¿¹ä¹‹ä¸€ã€‚
åœ¨ \hyperref[hashtable]{å“ˆå¸Œè¡¨} ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

\begin{exercise}

%ğŸ% Read the Wikipedia page on sorting algorithms at
%ğŸ% \url{http://en.wikipedia.org/wiki/Sorting_algorithm} and answer
%ğŸ% the following questions:
%ğŸ% \index{sorting}

é˜…è¯»\href{http://en.wikipedia.org/wiki/Sorting_algorithm}{æ’åºç®—æ³•}åœ¨ç»´åŸºç™¾ç§‘çš„ä»‹ç»ï¼Œå›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š
\index{sorting}
\index{ç»´åŸºç™¾ç§‘}


\begin{enumerate}

%ğŸ% \item What is a ``comparison sort?'' What is the best worst-case order
%ğŸ%   of growth for a comparison sort?  What is the best worst-case order
%ğŸ%   of growth for any sort algorithm?

\item ä»€ä¹ˆæ˜¯ ``\href{https://zh.wikipedia.org/wiki/%E6%AF%94%E8%BE%83%E6%8E%92%E5%BA%8F}{æ¯”è¾ƒæ’åº} (\href{https://en.wikipedia.org/wiki/Comparison_sort}{comparison sort})''ï¼Ÿ  æ¯”è¾ƒæ’åºæœ€ä¼˜æœ€å·®æƒ…å†µçš„å¢é•¿é‡çº§æ˜¯å¤šå°‘ï¼Ÿ  åˆ«çš„æ’åºç®—æ³•çš„ æœ€ä¼˜æœ€å·®æƒ…å†µçš„å¢é•¿é‡çº§ åˆæ˜¯å¤šå°‘ï¼Ÿ
\index{comparison sort}

%ğŸ% \item What is the order of growth of bubble sort, and why does Barack
%ğŸ%   Obama think it is ``the wrong way to go?''

\item å†’æ³¡æ’åºæ³•çš„å¢é•¿é‡çº§æ˜¯å¤šå°‘ï¼Ÿ ä¸ºä»€ä¹ˆå¥¥å·´é©¬è®¤ä¸ºæ˜¯``ä¸åº”é‡‡ç”¨çš„æ–¹æ³•''

%ğŸ% \item What is the order of growth of radix sort?  What preconditions
%ğŸ%   do we need to use it?

\item åŸºæ•°æ’åº(radix sort)çš„å¢é•¿é‡çº§æ˜¯å¤šå°‘ï¼Ÿ æˆ‘ä»¬ä½¿ç”¨å®ƒæ‰€éœ€è¦æ³¨æ„çš„å‰ææ¡ä»¶æœ‰å“ªäº›ï¼Ÿ

%ğŸ% \item What is a stable sort and why might it matter in practice?

\item æ’åºç®—æ³•çš„ç¨³å®šæ€§æ˜¯æŒ‡ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå®ƒåœ¨å®é™…æ“ä½œä¸­å¾ˆé‡è¦ï¼Ÿ
\index{stable sort}

%ğŸ% \item What is the worst sorting algorithm (that has a name)?

\item æœ€å·®çš„æ’åºç®—æ³•æ˜¯å“ªä¸€ä¸ªï¼ˆæœ‰åç§°çš„ï¼‰ï¼Ÿ

%ğŸ% \item What sort algorithm does the C library use?  What sort algorithm
%ğŸ%   does Python use?  Are these algorithms stable?  You might have to
%ğŸ%   Google around to find these answers.

\item C è¯­è¨€ä½¿ç”¨å“ªç§æ’åºç®—æ³•ï¼Ÿ Pythonä½¿ç”¨å“ªç§æ’åºç®—æ³•ï¼Ÿ è¿™äº›ç®—æ³•ç¨³å®šå—ï¼Ÿ ä½ å¯èƒ½éœ€è¦è°·æ­Œä¸€ä¸‹ï¼Œæ‰èƒ½æ‰¾åˆ°è¿™äº›ç­”æ¡ˆã€‚

%ğŸ% \item Many of the non-comparison sorts are linear, so why does does
%ğŸ%   Python use an $O(n \log n)$ comparison sort?

\item å¤§å¤šæ•° éæ¯”è¾ƒ ç®—æ³•æ˜¯çº¿æ€§çš„ï¼Œ å› æ­¤ä¸ºä»€ä¹ˆ Python ä½¿ç”¨ä¸€ä¸ª $O(n \log n)$ çš„ comparison sort ï¼Ÿ

\end{enumerate}

\end{exercise}


%ğŸ% \section{Analysis of search algorithms  |  æœç´¢ç®—æ³•åˆ†æ}
\section{æœç´¢ç®—æ³•åˆ†æ}

%ğŸ% A {\bf search} is an algorithm that takes a collection and a target
%ğŸ% item and determines whether the target is in the collection, often
%ğŸ% returning the index of the target.

{ \em æœç´¢} (search) ç®—æ³•ï¼Œå…¶æ¥å—ä¸€ä¸ªé›†åˆä»¥åŠä¸€ä¸ªç›®æ ‡é¡¹ï¼Œ
å¹¶å†³å®šè¯¥ç›®æ ‡é¡¹æ˜¯å¦åœ¨é›†åˆä¸­ï¼Œé€šå¸¸è¿”å›ç›®æ ‡çš„ç´¢å¼•å€¼ã€‚
\index{search}  \index{æœç´¢}

%ğŸ% The simplest search algorithm is a ``linear search'', which traverses
%ğŸ% the items of the collection in order, stopping if it finds the target.
%ğŸ% In the worst case it has to traverse the entire collection, so the run
%ğŸ% time is linear.

æœ€ç®€å•çš„æœç´ ç®—æ³•æ˜¯``çº¿æ€§æœç´¢''ï¼Œ å…¶æŒ‰é¡ºåºéå†é›†åˆä¸­çš„é¡¹ï¼Œ å¦‚æœæ‰¾åˆ°ç›®æ ‡åˆ™åœæ­¢ã€‚ æœ€åçš„æƒ…å†µä¸‹ï¼Œ å®ƒä¸å¾—ä¸éå†å…¨éƒ¨é›†åˆï¼Œ æ‰€ä»¥è¿è¡Œæ—¶é—´æ˜¯çº¿æ€§çš„ã€‚
\index{linear search}

%ğŸ% The {\tt in} operator for sequences uses a linear search; so do string
%ğŸ% methods like {\tt find} and {\tt count}.

åºåˆ—çš„ in è¿ç®—ç¬¦ä½¿ç”¨çº¿æ€§æœç´¢ã€‚ å­—ç¬¦ä¸²æ–¹æ³•ï¼Œå¦‚ \li{find} å’Œ \li{count} ä¹Ÿæ˜¯è¿™æ ·ã€‚
\index{in@{\tt in} operator}

%ğŸ% If the elements of the sequence are in order, you can use a {\bf
%ğŸ%   bisection search}, which is $O(\log n)$.  Bisection search is
%ğŸ% similar to the algorithm you might use to look a word up in a
%ğŸ% dictionary (a paper dictionary, not the data structure).  Instead of
%ğŸ% starting at the beginning and checking each item in order, you start
%ğŸ% with the item in the middle and check whether the word you are looking
%ğŸ% for comes before or after.  If it comes before, then you search the
%ğŸ% first half of the sequence.  Otherwise you search the second half.
%ğŸ% Either way, you cut the number of remaining items in half.

å¦‚æœå…ƒç´ åœ¨åºåˆ—ä¸­æ˜¯æ’åºå¥½çš„ï¼Œä½ å¯ä»¥ç”¨ {\em äºŒåˆ†æœç´ } (bisection search) ï¼Œ
å®ƒçš„å¢é•¿é‡çº§æ˜¯ $O(\log n)$ ã€‚  äºŒåˆ†æœç´¢å’Œä½ åœ¨å­—å…¸ä¸­æŸ¥æ‰¾ä¸€ä¸ªå•è¯çš„ç®—æ³•ç±»ä¼¼
ï¼ˆè¿™é‡Œæ˜¯æŒ‡çœŸæ­£çš„å­—å…¸ï¼Œä¸æ˜¯æ•°æ®ç»“æ„ï¼‰ã€‚ ä½ ä¸ä¼šä»å¤´å¼€å§‹å¹¶æŒ‰é¡ºåºæ£€æŸ¥æ¯ä¸ªé¡¹ï¼Œ
è€Œæ˜¯ä»ä¸­é—´çš„é¡¹å¼€å§‹å¹¶æ£€æŸ¥ä½ è¦æŸ¥æ‰¾çš„å•è¯åœ¨å‰é¢è¿˜æ˜¯åé¢ã€‚
å¦‚æœå®ƒå‡ºç°åœ¨å‰é¢ï¼Œé‚£ä¹ˆä½ æœç´¢åºåˆ—çš„å‰åŠéƒ¨åˆ†ã€‚
å¦åˆ™ä½ æœç´¢åä¸€åŠã€‚ å¦‚è®ºå¦‚ä½•ï¼Œä½ å°†å‰©ä½™çš„é¡¹æ•°åˆ†ä¸ºä¸€åŠã€‚
\index{bisection search}  \index{äºŒåˆ†æœç´ }

%ğŸ% If the sequence has 1,000,000 items, it will take about 20 steps to
%ğŸ% find the word or conclude that it's not there.  So that's about 50,000
%ğŸ% times faster than a linear search.

å¦‚æœåºåˆ—æœ‰ 1,000,000 é¡¹ï¼Œå®ƒå°†èŠ± 20 æ­¥æ‰¾åˆ°è¯¥å•è¯æˆ–è¯´æ‰¾ä¸åˆ°ã€‚
å› æ­¤å®ƒæ¯”çº¿æ€§æœç´¢å¿«å¤§æ¦‚ 50,000 å€ã€‚

%ğŸ% Bisection search can be much faster than linear search, but
%ğŸ% it requires the sequence to be in order, which might require
%ğŸ% extra work.

äºŒåˆ†æœç´¢æ¯”çº¿æ€§æœç´¢å¿«å¾ˆå¤šï¼Œä½†æ˜¯å®ƒè¦æ±‚å·²æ’åºçš„åºåˆ—ï¼Œå› æ­¤ä½¿ç”¨æ—¶ä¼šæœ‰é¢å¤–çš„å·¥ä½œè¦åšã€‚

%ğŸ% There is another data structure, called a {\bf hashtable} that
%ğŸ% is even faster---it can do a search in constant time---and it
%ğŸ% doesn't require the items to be sorted.  Python dictionaries
%ğŸ% are implemented using hashtables, which is why most dictionary
%ğŸ% operations, including the {\tt in} operator, are constant time.

å¦ä¸€ä¸ªæ£€ç´¢é€Ÿåº¦æ›´å¿«çš„æ•°æ®ç»“æ„è¢«ç§°ä¸º {\em å“ˆå¸Œè¡¨} (hashtable) --- å®ƒå¯ä»¥åœ¨ å¸¸æ•°æ—¶é—´ å†…æ£€ç´¢å‡ºç»“æœ --- å¹¶ä¸”ä¸ä¾èµ–äºåºåˆ—æ˜¯å¦å·²æ’åºã€‚ Python çš„å†…å»ºå­—å…¸å°±é€šè¿‡å“ˆå¸Œè¡¨æŠ€æœ¯è€Œå®ç°ï¼Œå› æ­¤å¤§å¤šæ•°çš„å­—å…¸æ“ä½œï¼ŒåŒ…æ‹¬ \li{in} ï¼ŒåªèŠ±è´¹å¸¸æ•°æ—¶é—´å°±å¯å®Œæˆã€‚


%ğŸ% \section{Hashtables  |  å“ˆå¸Œè¡¨}
\section{å“ˆå¸Œè¡¨}
\label{hashtable}

%ğŸ% To explain how hashtables work and why their performance is so
%ğŸ% good, I start with a simple implementation of a map and
%ğŸ% gradually improve it until it's a hashtable.

ä¸ºäº†è§£é‡Šå“ˆå¸Œè¡¨æ˜¯å¦‚ä½•å·¥ä½œï¼Œä»¥åŠä¸ºä»€ä¹ˆå®ƒçš„æ€§èƒ½éå¸¸ä¼˜ç§€ï¼Œ
æˆ‘ä»¬ä»å®ç°ä¸€ä¸ªç®€å•çš„æ˜ å°„ (map) å¼€å§‹å¹¶é€æ­¥æ”¹è¿›å®ƒï¼Œç›´åˆ°æˆä¸ºä¸€ä¸ªå“ˆå¸Œè¡¨ã€‚
\index{hashtable}  \index{å“ˆå¸Œè¡¨}

%ğŸ% I use Python to demonstrate these implementations, but in real
%ğŸ% life you wouldn't write code like this in Python; you would just use a
%ğŸ% dictionary!  So for the rest of this chapter, you have to imagine that
%ğŸ% dictionaries don't exist and you want to implement a data structure
%ğŸ% that maps from keys to values.  The operations you have to
%ğŸ% implement are:

æˆ‘ä»¬ä½¿ç”¨ Python æ¥æ¼”ç¤ºè¿™äº›å®ç°æ¡ˆä¾‹ï¼Œ ä½†åœ¨ç°å®æƒ…å†µä¸‹ï¼Œ
ä½ ä¸éœ€è¦ç”¨ Python äº²è‡ªå†™è¿™æ ·çš„ä»£ç ï¼Œ åªéœ€ç”¨å†…å»ºçš„å­—å…¸å¯¹è±¡å³å¯ï¼
å› æ­¤ä»¥ä¸‹çš„å†…å®¹ï¼Œæ˜¯åŸºäºä¸å­˜åœ¨æˆ‘ä»¬éœ€è¦çš„å­—å…¸å¯¹è±¡çš„å‡è®¾ï¼Œ
æˆ‘ä»¬æƒ³å®ç°è¿™æ ·ä¸€ä¸ªæ•°æ®ç»“æ„ï¼Œå°†å…³é”®å­—æ˜ å°„ä¸ºå€¼ã€‚ ä½ éœ€è¦å®ç°å¦‚ä¸‹çš„å‡½æ•°è¿ç®—ï¼š

%ğŸ% \begin{description}
%ğŸ%
%ğŸ% \item[{\tt add(k, v)}:] Add a new item that maps from key {\tt k}
%ğŸ% to value {\tt v}.  With a Python dictionary, {\tt d}, this operation
%ğŸ% is written {\tt d[k] = v}.
%ğŸ%
%ğŸ% \item[{\tt get(k)}:] Look up and return the value that corresponds
%ğŸ% to key {\tt k}.  With a Python dictionary, {\tt d}, this operation
%ğŸ% is written {\tt d[k]} or {\tt d.get(k)}.
%ğŸ%
%ğŸ% \end{description}

\begin{description}

\item [{\tt add(k, v)}:] å¢åŠ ä¸€ä¸ªæ–°çš„é¡¹ï¼Œå…¶ä»é”® \li{k} æ˜ å°„åˆ°å€¼ \li{v}ã€‚
ä½¿ç”¨ Python çš„å­—å…¸ \li{d}ï¼Œè¯¥è¿ç®—è¢«å†™ä½œ \li{d[k] = v}ã€‚

\item [{\tt get(k)}:] æŸ¥æ‰¾å¹¶è¿”å›ç›¸åº”é”®çš„å€¼ã€‚
å¦‚æœä½¿ç”¨ Python çš„å­—å…¸ \li{d} ï¼Œè¯¥è¿ç®—è¢«å†™ä½œ \li{d[target]} æˆ– \li{d.get(target)}ã€‚

\end{description}

%ğŸ% For now, I assume that each key only appears once.
%ğŸ% The simplest implementation of this interface uses a list of
%ğŸ% tuples, where each tuple is a key-value pair.

ç°åœ¨ï¼Œå‡è®¾æ¯ä¸ªå…³é”®å­—åªå‡ºç°ä¸€æ¬¡ã€‚
è¯¥æ¥å£æœ€ç®€å•çš„å®ç°æ˜¯ä½¿ç”¨ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œ å…¶ä¸­æ¯ä¸ªå…ƒç»„æ˜¯å…³é”®å­—-å€¼å¯¹ã€‚
\index{LinearMap@{\tt LinearMap}}

\begin{lstlisting}
class LinearMap:

    def __init__(self):
        self.items = []

    def add(self, k, v):
        self.items.append((k, v))

    def get(self, k):
        for key, val in self.items:
            if key == k:
                return val
        raise KeyError
\end{lstlisting}

%ğŸ% {\tt add} appends a key-value tuple to the list of items, which
%ğŸ% takes constant time.

\li{add} å‘é¡¹åˆ—è¡¨è¿½åŠ ä¸€ä¸ªå…³é”®å­—---å€¼å…ƒç»„ï¼Œè¿™æ˜¯å¸¸æ•°æ—¶é—´ã€‚

%ğŸ% {\tt get} uses a {\tt for} loop to search the list:
%ğŸ% if it finds the target key it returns the corresponding value;
%ğŸ% otherwise it raises a {\tt KeyError}.
%ğŸ% So {\tt get} is linear.

\index{KeyError@{\tt KeyError}}

\li{get} ä½¿ç”¨ \li{for} å¾ªç¯æœç´¢è¯¥åˆ—è¡¨ï¼š å¦‚æœå®ƒæ‰¾åˆ°ç›®æ ‡å…³é”®å­—ï¼Œè¿”å›ç›¸åº”çš„å€¼ï¼›
å¦åˆ™è§¦å‘ä¸€ä¸ªKeyErrorã€‚å› æ­¤getæ˜¯çº¿æ€§çš„ã€‚

%ğŸ% An alternative is to keep the list sorted by key.  Then {\tt get}
%ğŸ% could use a bisection search, which is $O(\log n)$.  But inserting a
%ğŸ% new item in the middle of a list is linear, so this might not be the
%ğŸ% best option.  There are other data structures that can implement {\tt
%ğŸ%   add} and {\tt get} in log time, but that's still not as good as
%ğŸ% constant time, so let's move on.

å¦ä¸€ä¸ªæ–¹æ¡ˆæ˜¯ä¿æŒåˆ—è¡¨æŒ‰å…³é”®å­—æ’åºã€‚
é‚£ä¹ˆ \li{get} å¯ä»¥ä½¿ç”¨äºŒåˆ†æœç´¢ï¼Œå…¶æ˜¯ $O(\log n)$ ã€‚
ä½†æ˜¯åœ¨åˆ—è¡¨ä¸­é—´æ’å…¥ä¸€ä¸ªæ–°çš„é¡¹æ˜¯çº¿æ€§çš„ï¼Œ å› æ­¤è¿™å¯èƒ½ä¸æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚
æœ‰å…¶å®ƒçš„æ•°æ®ç»“æ„ï¼ˆè§ï¼š\href{http://en.wikipedia.org/wiki/Red-black_tree}{ç»´åŸºç™¾ç§‘}ï¼‰
èƒ½åœ¨ å¯¹æ•°çº§ æ—¶é—´å†…å®ç° \li{add} å’Œ \li{get}ï¼Œä½†æ˜¯è¿™ä»ç„¶ä¸å¦‚å¸¸æ•°æ—¶é—´å¥½ï¼Œ
é‚£ä¹ˆæˆ‘ä»¬ç»§ç»­ã€‚
\index{red-black tree}

\index{ç»´åŸºç™¾ç§‘}


%ğŸ% One way to improve {\tt LinearMap} is to break the list of key-value
%ğŸ% pairs into smaller lists.  Here's an implementation called
%ğŸ% {\tt BetterMap}, which is a list of 100 LinearMaps.  As we'll see
%ğŸ% in a second, the order of growth for {\tt get} is still linear,
%ğŸ% but {\tt BetterMap} is a step on the path toward hashtables:

\index{BetterMap@{\tt BetterMap}}

å¦ä¸€ç§æ”¹è‰¯ \li{LinearMap} çš„æ–¹æ³•æ˜¯ å°† å…³é”®å­—-å€¼ å¯¹çš„åˆ—è¡¨åˆ†æˆå°åˆ—è¡¨ã€‚
è¿™æ˜¯ä¸€ä¸ªè¢«ç§°ä½œ \li{BetterMap} çš„æ›´å¥½çš„å®ç°ï¼Œå®ƒæ˜¯ 100 ä¸ª \li{LinearMaps} çš„åˆ—è¡¨ã€‚
æ­£å¦‚ä¸€ä¼šå„¿æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œ\li{get} çš„å¢é•¿é‡çº§ä»ç„¶æ˜¯çº¿æ€§çš„ï¼Œ
ä½†æ˜¯ \li{BetterMap} æ˜¯è¿ˆå‘å“ˆå¸Œè¡¨çš„ä¸€æ­¥ã€‚

\begin{lstlisting}
class BetterMap:

    def __init__(self, n=100):
        self.maps = []
        for i in range(n):
            self.maps.append(LinearMap())

    def find_map(self, k):
        index = hash(k) % len(self.maps)
        return self.maps[index]

    def add(self, k, v):
        m = self.find_map(k)
        m.add(k, v)

    def get(self, k):
        m = self.find_map(k)
        return m.get(k)
\end{lstlisting}

%ğŸ% \verb"__init__" makes a list of {\tt n} {\tt LinearMap}s.

\li{__init__} ä¼šç”Ÿæˆä¸€ä¸ªç”± \li{n} ä¸ª \li{LinearMap} ç»„æˆçš„åˆ—è¡¨ã€‚

%ğŸ% \verb"find_map" is used by
%ğŸ% {\tt add} and {\tt get}
%ğŸ% to figure out which map to put the
%ğŸ% new item in, or which map to search.

\li{add} å’Œ \li{get} ä½¿ç”¨ \li{find_map} æŸ¥æ‰¾å¾€å“ªä¸€ä¸ªåˆ—è¡¨ä¸­æ·»åŠ æ–°é¡¹ï¼Œæˆ–è€…å¯¹å“ªä¸ªåˆ—è¡¨è¿›è¡Œæ£€ç´¢ã€‚


%ğŸ% \verb"find_map" uses the built-in function {\tt hash}, which takes
%ğŸ% almost any Python object and returns an integer.  A limitation of this
%ğŸ% implementation is that it only works with hashable keys.  Mutable
%ğŸ% types like lists and dictionaries are unhashable.

\li{find_map} ä½¿ç”¨å†…å»º \li{hash} å‡½æ•°ï¼Œå…¶æ¥å—å‡ ä¹ä»»ä½• Python å¯¹è±¡å¹¶è¿”å›ä¸€ä¸ªæ•´æ•°ã€‚
è¿™ä¸€å®ç°çš„ä¸€ä¸ªé™åˆ¶æ˜¯å®ƒä»…é€‚ç”¨äºå“ˆå¸Œè¡¨å…³é”®å­—ã€‚ å¦‚åˆ—è¡¨å’Œå­—å…¸ç­‰æ˜“å˜çš„ç±»å‹æ˜¯ä¸èƒ½å“ˆå¸Œçš„ã€‚
\index{hash function}

%ğŸ% Hashable objects that are considered equivalent return the same hash
%ğŸ% value, but the converse is not necessarily true: two objects with
%ğŸ% different values can return the same hash value.

è¢«è®¤ä¸ºæ˜¯ç›¸ç­‰çš„å¯å“ˆå¸Œçš„å¯¹è±¡è¿”å›ç›¸åŒçš„å“ˆå¸Œå€¼ï¼Œ
ä½†æ˜¯åä¹‹ä¸å¿…æˆç«‹ï¼šä¸¤ä¸ªä¸åŒçš„å¯¹è±¡èƒ½å¤Ÿè¿”å›ç›¸åŒçš„å“ˆå¸Œå€¼ã€‚

%ğŸ% \verb"find_map" uses the modulus operator to wrap the hash values
%ğŸ% into the range from 0 to {\tt len(self.maps)}, so the result is a legal
%ğŸ% index into the list.  Of course, this means that many different
%ğŸ% hash values will wrap onto the same index.  But if the hash function
%ğŸ% spreads things out pretty evenly (which is what hash functions
%ğŸ% are designed to do), then we expect $n/100$ items per LinearMap.

\li{find_map} ä½¿ç”¨æ±‚ä½™è¿ç®—ç¬¦å°†å“ˆå¸Œå€¼åŒ…åœ¨ 0 åˆ° \li{len(self.maps)} ä¹‹é—´ï¼Œ
å› æ­¤ç»“æœæ˜¯å¯¹äºè¯¥åˆ—è¡¨åˆæ³•çš„ç´¢å¼•å€¼ã€‚ å½“ç„¶ï¼Œè¿™æ„å‘³ç€è®¸å¤šä¸åŒçš„å“ˆå¸Œå€¼å°†è¢«åŒ…æˆç›¸åŒçš„ç´¢å¼•å€¼ã€‚ ä½†æ˜¯å¦‚æœå“ˆå¸Œå‡½æ•°æ•£å¸ƒç›¸å½“å‡åŒ€ï¼ˆè¿™æ˜¯å“ˆå¸Œå‡½æ•°è¢«è®¾è®¡çš„åˆè¡·ï¼‰ï¼Œ é‚£ä¹ˆæˆ‘ä»¬é¢„è®¡æ¯ä¸ª \li{LinearMap} æœ‰ $n/100$ é¡¹ã€‚

%ğŸ% Since the run time of {\tt LinearMap.get} is proportional to the
%ğŸ% number of items, we expect BetterMap to be about 100 times faster
%ğŸ% than LinearMap.  The order of growth is still linear, but the
%ğŸ% leading coefficient is smaller.  That's nice, but still not
%ğŸ% as good as a hashtable.

ç”±äº \li{LinearMap.get} çš„è¿è¡Œæ—¶é—´ä¸é¡¹æ•°æˆæ­£æ¯”ï¼Œ
æˆ‘ä»¬é¢„è®¡ \li{BetterMap} æ¯” \li{LinearMap} å¿«100å€ã€‚
å¢é•¿é‡çº§ä»ç„¶æ˜¯çº¿æ€§çš„ï¼Œä½†æ˜¯é¦–ç³»æ•°å˜å°äº†ã€‚ è¿™å¾ˆå¥½ï¼Œä½†æ˜¯ä»ç„¶ä¸å¦‚å“ˆå¸Œè¡¨å¥½ã€‚

%ğŸ% Here (finally) is the crucial idea that makes hashtables fast: if you
%ğŸ% can keep the maximum length of the LinearMaps bounded, {\tt
%ğŸ%   LinearMap.get} is constant time.  All you have to do is keep track
%ğŸ% of the number of items and when the number of
%ğŸ% items per LinearMap exceeds a threshold, resize the hashtable by
%ğŸ% adding more LinearMaps.

ä¸‹é¢æ˜¯ä½¿å“ˆå¸Œè¡¨å˜å¿«çš„å…³é”®ï¼š
å¦‚æœä½ èƒ½ä¿è¯ \li{LinearMaps} çš„æœ€å¤§é•¿åº¦æ˜¯æœ‰ä¸Šé™çš„ï¼Œåˆ™ \li{LinearMap.get} çš„å¢é•¿é‡çº§æ˜¯å¸¸æ•°æ—¶é—´ã€‚
ä½ åªéœ€è¦è·Ÿè¸ªé¡¹æ•°å¹¶ä¸”å½“æ¯ä¸ª \li{LinearMap} çš„é¡¹æ•°è¶…è¿‡ä¸€ä¸ªé˜ˆå€¼æ—¶ï¼Œ
é€šè¿‡å¢åŠ æ›´å¤šçš„ \li{LinearMaps} è°ƒæ•´å“ˆå¸Œè¡¨çš„å¤§å°ã€‚
\index{bounded}

%ğŸ% Here is an implementation of a hashtable:

è¿™æ˜¯å“ˆå¸Œè¡¨çš„ä¸€ä¸ªå®ç°ï¼š
\index{HashMap}

\begin{lstlisting}
class HashMap:

    def __init__(self):
        self.maps = BetterMap(2)
        self.num = 0

    def get(self, k):
        return self.maps.get(k)

    def add(self, k, v):
        if self.num == len(self.maps.maps):
            self.resize()

        self.maps.add(k, v)
        self.num += 1

    def resize(self):
        new_maps = BetterMap(self.num * 2)

        for m in self.maps.maps:
            for k, v in m.items:
                new_maps.add(k, v)

        self.maps = new_maps
\end{lstlisting}

%ğŸ% Each {\tt HashMap} contains a {\tt BetterMap}; \verb"__init__" starts
%ğŸ% with just 2 LinearMaps and initializes {\tt num}, which keeps track of
%ğŸ% the number of items.

æ¯ä¸ª \li{HashMap} åŒ…å«ä¸€ä¸ª \li{BetterMap}ã€‚
\li{__init__} å¼€å§‹ä»…æœ‰ä¸¤ä¸ª \li{LinearMaps}ï¼Œ å¹¶ä¸”åˆå§‹åŒ– \li{num}ï¼Œ ç”¨äºè·Ÿè¸ªé¡¹çš„æ•°ç›®ã€‚

%ğŸ% {\tt get} just dispatches to {\tt BetterMap}.  The real work happens
%ğŸ% in {\tt add}, which checks the number of items and the size of the
%ğŸ% {\tt BetterMap}: if they are equal, the average number of items per
%ğŸ% LinearMap is 1, so it calls {\tt resize}.

\li{get} ä»…ä»…ç”¨æ¥è°ƒåº¦ \li{BetterMap}ã€‚
çœŸæ­£çš„æ“ä½œå‘ç”Ÿäº \li{add} å†…ï¼Œå…¶æ£€æŸ¥é¡¹çš„æ•°é‡ä»¥åŠ \li{BetterMap} çš„å¤§å°ï¼š
å¦‚æœå®ƒä»¬ç›¸åŒï¼Œæ¯ä¸ª \li{LinearMap} çš„å¹³å‡é¡¹æ•°ä¸º 1ï¼Œå› æ­¤å®ƒè°ƒç”¨ \li{resize}ã€‚

%ğŸ% {\tt resize} make a new {\tt BetterMap}, twice as big as the previous
%ğŸ% one, and then ``rehashes'' the items from the old map to the new.

\li{resize} ç”Ÿæˆä¸€ä¸ªæ–°çš„ \li{BetterMap}ï¼Œæ˜¯ä¹‹å‰çš„ä¸¤å€å¤§ï¼Œ
ç„¶åå°†åƒä»æ—§è¡¨\li{map} ``é‡æ–°å“ˆå¸Œ'' è‡³æ–°çš„è¡¨ã€‚

%ğŸ% Rehashing is necessary because changing the number of LinearMaps
%ğŸ% changes the denominator of the modulus operator in
%ğŸ% \verb"find_map".  That means that some objects that used
%ğŸ% to hash into the same LinearMap will get split up (which is
%ğŸ% what we wanted, right?).

é‡å“ˆå¸Œæ˜¯å¾ˆå¿…è¦çš„ï¼Œå› ä¸ºæ”¹å˜ \li{LinearMaps} çš„æ•°ç›®ä¹Ÿæ”¹å˜äº† \li{find_map} ä¸­æ±‚ä½™è¿ç®—çš„åˆ†æ¯ã€‚
é‚£æ„å‘³ç€ä¸€äº›è¢«åŒ…è¿›ç›¸åŒçš„ \li{LinearMap} çš„å¯¹è±¡å°†è¢«åˆ†ç¦»ï¼ˆè¿™æ­£æ˜¯æˆ‘ä»¬å¸Œæœ›çš„ï¼Œå¯¹å§ï¼Ÿï¼‰
\index{rehashing}

%ğŸ% Rehashing is linear, so
%ğŸ% {\tt resize} is linear, which might seem bad, since I promised
%ğŸ% that {\tt add} would be constant time.  But remember that
%ğŸ% we don't have to resize every time, so {\tt add} is usually
%ğŸ% constant time and only occasionally linear.  The total amount
%ğŸ% of work to run {\tt add} $n$ times is proportional to $n$,
%ğŸ% so the average time of each {\tt add} is constant time!

é‡å“ˆå¸Œæ˜¯çº¿æ€§çš„ï¼Œå› æ­¤ \li{resize} æ˜¯çº¿æ€§çš„ï¼Œ
è¿™å¯èƒ½çœ‹èµ·æ¥å¾ˆç³Ÿç³•ï¼Œå› ä¸ºæˆ‘ä¿è¯ \li{add} ä¼šæ˜¯å¸¸æ•°æ—¶é—´ã€‚
ä½†æ˜¯è®°ä½ï¼Œæˆ‘ä»¬ä¸å¿…æ²¡æ¯æ¬¡éƒ½è°ƒæ•´ï¼Œå› æ­¤ \li{add} é€šå¸¸æ˜¯å¸¸æ•°æ—¶é—´
åªæ˜¯å¶ç„¶æ˜¯çº¿æ€§çš„ã€‚  è¿è¡Œ \li{add} $n$ æ¬¡çš„æ•´ä½“æ“ä½œé‡ä¸ $n$ æˆæ­£æ¯”ï¼Œ
å› æ­¤ \li{add} çš„å¹³å‡è¿è¡Œæ—¶é—´æ˜¯å¸¸æ•°æ—¶é—´ï¼
\index{constant time}

%ğŸ% To see how this works, think about starting with an empty
%ğŸ% HashTable and adding a sequence of items.  We start with 2 LinearMaps,
%ğŸ% so the first 2 adds are fast (no resizing required).  Let's
%ğŸ% say that they take one unit of work each.  The next add
%ğŸ% requires a resize, so we have to rehash the first two
%ğŸ% items (let's call that 2 more units of work) and then
%ğŸ% add the third item (one more unit).  Adding the next item
%ğŸ% costs 1 unit, so the total so far is
%ğŸ% 6 units of work for 4 items.

ä¸ºäº†å¼„æ¸…è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè€ƒè™‘ä»¥ä¸€ä¸ªç©ºçš„ \li{HashTable} å¼€å§‹å¹¶å¢åŠ ä¸€ç³»åˆ—é¡¹ã€‚
æˆ‘ä»¬ä»¥ä¸¤ä¸ª \li{LinearMap} å¼€å§‹ï¼Œå› æ­¤å‰ä¸¤ä¸ª \li{add} å¾ˆå¿«ï¼ˆä¸éœ€è¦è°ƒæ•´å¤§å° \li{resize}ï¼‰ã€‚
æˆ‘ä»¬å‡è®¾å®ƒä»¬æ¯ä¸ªæ“ä½œèŠ±è´¹ä¸€ä¸ªå·¥ä½œå•å…ƒã€‚
ä¸‹ä¸€ä¸ª \li{add} éœ€è¦è¿›è¡Œä¸€æ¬¡ \li{resize}ï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»é‡å“ˆå¸Œå‰ä¸¤é¡¹
ï¼ˆæˆ‘ä»¬å°†å…¶ç®—ä½œè°ƒç”¨ä¸¤ä¸ªé¢å¤–çš„å·¥ä½œå•å…ƒï¼‰ï¼Œç„¶åå¢åŠ ç¬¬ 3 é¡¹ï¼ˆåˆä¸€ä¸ªé¢å¤–å•å…ƒï¼‰ã€‚
å¢åŠ ä¸‹ä¸€é¡¹èŠ±è´¹ 1 ä¸ªå•å…ƒï¼Œæ‰€ä»¥ç›®å‰ä¸ºæ­¢æ·»åŠ 4ä¸ªé¡¹æ€»å…±éœ€è¦6ä¸ªå•å…ƒã€‚

%ğŸ% The next {\tt add} costs 5 units, but the next three
%ğŸ% are only one unit each, so the total is 14 units for the
%ğŸ% first 8 adds.

ä¸‹ä¸€ä¸ª \li{add} èŠ±è´¹5ä¸ªå•å…ƒï¼Œä½†æ˜¯ä¹‹åçš„3ä¸ªæ“ä½œæ¯ä¸ªåªèŠ±è´¹ 1 ä¸ªå•å…ƒï¼Œ
æ‰€ä»¥å‰ 8 ä¸ª \li{add} æ€»å…±éœ€è¦ 14 ä¸ªå•å…ƒã€‚

%ğŸ% The next {\tt add} costs 9 units, but then we can add 7 more
%ğŸ% before the next resize, so the total is 30 units for the
%ğŸ% first 16 adds.

ä¸‹ä¸€ä¸ª \li{add} èŠ±è´¹9ä¸ªå•å…ƒï¼Œä½†æ˜¯ä¹‹ååœ¨ä¸‹ä¸€æ¬¡ è°ƒæ•´å¤§å° (\li{resize})
ä¹‹å‰ï¼Œå¯ä»¥å†å¢åŠ é¢å¤–çš„7ä¸ªï¼Œæ‰€ä»¥å‰16ä¸ªaddæ€»å…±æ˜¯30ä¸ªå•å…ƒã€‚

%ğŸ% After 32 adds, the total cost is 62 units, and I hope you are starting
%ğŸ% to see a pattern.  After $n$ adds, where $n$ is a power of two, the
%ğŸ% total cost is $2n-2$ units, so the average work per add is
%ğŸ% a little less than 2 units.  When $n$ is a power of two, that's
%ğŸ% the best case; for other values of $n$ the average work is a little
%ğŸ% higher, but that's not important.  The important thing is that it
%ğŸ% is $O(1)$.

åœ¨ 32 æ¬¡ \li{add} åï¼Œæ€»å…±èŠ±è´¹ 62 ä¸ªå•å…ƒï¼Œæˆ‘æƒ³ä½ å¼€å§‹çœ‹åˆ°ä¸€ä¸ªè§„å¾‹ã€‚
$n$ æ¬¡ add åï¼Œå…¶ä¸­ $n$ æ˜¯ 2 çš„æŒ‡æ•°ï¼Œæ€»å…±èŠ±è´¹æ˜¯ $2n-2$ ä¸ªå•å…ƒï¼Œ
æ‰€ä»¥å¹³å‡æ¯ä¸ª \li{add} æ“ä½œçš„èŠ±è´¹ç¨å¾®å°‘äº 2 ä¸ªå•å…ƒã€‚
å½“ $n$ æ˜¯ 2 çš„æŒ‡æ•°æ—¶æ˜¯æœ€å¥½çš„æƒ…å†µã€‚
å¯¹äºå…¶å®ƒçš„ $n$ å€¼ï¼Œå¹³å‡èŠ±è´¹ç¨é«˜ä¸€ç‚¹ï¼Œä½†æ˜¯é‚£å¹¶ä¸é‡è¦ã€‚
é‡è¦çš„æ˜¯å…¶å¢é•¿é‡çº§ä¸º $O(1)$ ã€‚
\index{average cost}

%ğŸ% Figure~\ref{fig.hash} shows how this works graphically.  Each
%ğŸ% block represents a unit of work.  The columns show the total
%ğŸ% work for each add in order from left to right: the first two
%ğŸ% {\tt adds} cost 1 units, the third costs 3 units, etc.

å›¾~\ref{fig.hash}å±•ç¤ºè¿™å¦‚ä½•å·¥ä½œçš„ã€‚
æ¯ä¸ªå—ä»£è¡¨ä¸€ä¸ªå·¥ä½œå•å…ƒã€‚æŒ‰ä»å·¦åˆ°å³çš„é¡ºåºï¼Œæ¯åˆ—æ˜¾ç¤ºæ¯ä¸ªaddæ‰€éœ€çš„å•å…ƒï¼š
å‰ä¸¤ä¸ªaddsèŠ±è´¹1ä¸ªå•å…ƒï¼Œç¬¬3ä¸ªèŠ±è´¹3ä¸ªå•å…ƒç­‰ç­‰ã€‚

\begin{figure}
\centerline{\includegraphics[width=5.5in]{../source/figs/towers.pdf}}
% \caption{The cost of a hashtable add.\label{fig.hash}}
\caption{ \li{add} æ“ä½œåœ¨å“ˆå¸Œè¡¨ä¸­çš„æˆæœ¬ã€‚ \label{fig.hash}}
\end{figure}

%ğŸ% The extra work of rehashing appears as a sequence of increasingly
%ğŸ% tall towers with increasing space between them.  Now if you knock
%ğŸ% over the towers, spreading the cost of resizing over all
%ğŸ% adds, you can see graphically that the total cost after $n$
%ğŸ% adds is $2n - 2$.

é‡æ–°å“ˆå¸Œçš„é¢å¤–å·¥ä½œï¼Œè¡¨ç°ä¸ºä¸€ç³»åˆ—ä¸æ–­å¢é«˜çš„é«˜å¡”ï¼Œå„è‡ªä¹‹é—´çš„è·ç¦»è¶Šæ¥è¶Šå¤§ã€‚
ç°åœ¨ï¼Œå¦‚æœä½ æ‰“ç¿»è¿™äº›å¡”ï¼Œå°† è°ƒæ•´å¤§å° (\li{resize}) çš„ä»£ä»·å‡æ‘Šåˆ°æ‰€æœ‰çš„ \li{add}æ“ä½œä¸Šï¼Œ ä½ ä¼šä»å›¾ä¸Šçœ‹åˆ° $n$ ä¸ª \li{add} çš„æ•´ä½“èŠ±è´¹æ˜¯ $2n - 2$ ã€‚

%ğŸ% An important feature of this algorithm is that when we resize the
%ğŸ% HashTable it grows geometrically; that is, we multiply the size by a
%ğŸ% constant.  If you increase the size
%ğŸ% arithmetically---adding a fixed number each time---the average time
%ğŸ% per {\tt add} is linear.

è¯¥ç®—æ³•ä¸€ä¸ªé‡è¦çš„ç‰¹å¾æ˜¯å½“æˆ‘ä»¬ è°ƒæ•´ (\li{resize}) å“ˆå¸Œè¡¨ (\li{HashTable})
å¤§å°çš„æ—¶å€™ï¼Œå®ƒå‘ˆå‡ ä½•çº§å¢é•¿ã€‚  ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ç”¨å¸¸æ•°ä¹˜ä»¥è¡¨çš„å¤§å°ã€‚
å¦‚æœä½ æŒ‰ç®—æœ¯çº§å¢åŠ å¤§å° â€”â€” æ¯æ¬¡å¢åŠ å›ºå®šçš„æ•°ç›® â€”â€” æ¯ä¸ª \li{add} æ“ä½œçš„å¹³å‡æ—¶é—´æ˜¯çº¿æ€§çš„ã€‚
\index{geometric resizing}

%ğŸ% You can download my implementation of HashMap from
%ğŸ% \url{http://thinkpython2.com/code/Map.py}, but remember that there
%ğŸ% is no reason to use it; if you want a map, just use a Python dictionary.

ä½ å¯ä»¥ \href{http://thinkpython2.com/code/Map.py}{åœ¨æ­¤}ä¸‹è½½ åˆ° \li{HashMap} çš„å®ç°ä»£ç ï¼Œ ä½†åœ¨å®é™…ç¯å¢ƒä¸­ç›´æ¥ä½¿ç”¨ Python çš„{\em å­—å…¸} å³å¯ ã€‚

%ğŸ% \section{Glossary  |  æœ¯è¯­è¡¨}
\section{æœ¯è¯­è¡¨}

\begin{description}

%ğŸ% \item[analysis of algorithms:] A way to compare algorithms in terms of
%ğŸ% their run time and/or space requirements.

\item[ç®—æ³•åˆ†æ (analysis of algorithms)] æ¯”è¾ƒä¸åŒç®—æ³•é—´è¿è¡Œæ—¶é—´å’Œèµ„æºå ç”¨çš„åˆ†ææ–¹æ³•ã€‚
\index{analysis of algorithms}

%ğŸ% \item[machine model:] A simplified representation of a computer used
%ğŸ% to describe algorithms.

\item[æ¨¡å‹æœºå™¨ (machine model)] ç”¨äºæè¿°ç®—æ³•ï¼ˆæ€§èƒ½ï¼‰çš„ç®€åŒ–çš„è®¡ç®—æœºè¡¨ç¤ºã€‚
\index{machine model}

%ğŸ% \item[worst case:] The input that makes a given algorithm run slowest (or
%ğŸ% require the most space.

\item[æœ€åæƒ…å†µ (worest case)] å¯¹ç»™å®šç®—æ³•è¯æœ€é•¿æ—¶é—´è¿è¡Œï¼ˆæˆ–å ç”¨åšå¤šèµ„æºï¼‰çš„è¾“å…¥ã€‚
\index{worst case}

%ğŸ% \item[leading term:] In a polynomial, the term with the highest exponent.

\item[é¦–é¡¹ (leading term)] åœ¨å¤šé¡¹å¼ä¸­ï¼Œæ‹¥æœ‰æŒ‡æ•°æœ€é«˜çš„é¡¹ã€‚
\index{leading term}

%ğŸ% \item[crossover point:] The problem size where two algorithms require
%ğŸ% the same run time or space.

\item[äº¤å‰ç‚¹ (crossover point)] å¯¹ç»™å®šé—®é¢˜çš„æŸä¸ªè§„æ¨¡ï¼Œä½¿å¾—ä¸¤ä¸ªç®—æ³•çš„æ±‚è§£éœ€è¦ç›¸åŒè¿è¡Œæ—¶é—´æˆ–èµ„æºå¼€é”€ã€‚
\index{crossover point}

%ğŸ% \item[order of growth:] A set of functions that all grow in a way
%ğŸ% considered equivalent for purposes of analysis of algorithms.
%ğŸ% For example, all functions that grow linearly belong to the same
%ğŸ% order of growth.

\item [å¢é•¿é‡çº§ (order of growth)] ä¸€ä¸ªå‡½æ•°çš„é›†åˆï¼Œä»ç®—æ³•åˆ†æçš„è§’åº¦æ¥çœ‹å…¶ä¸­çš„å‡½æ•°çš„å¢é•¿è§†ä¸ºç­‰ä»·çš„ã€‚ ä¾‹å¦‚ï¼Œ çº¿æ€§é€’å¢çš„æ‰€æœ‰çš„å‡½æ•°éƒ½ å±äº åŒä¸€ä¸ªå¢é•¿é‡çº§ã€‚
\index{order of growth}

%ğŸ% \item[Big-Oh notation:] Notation for representing an order of growth;
%ğŸ% for example, $O(n)$ represents the set of functions that grow
%ğŸ% linearly.

\item[å¤§Oç¬¦å· (Big-Oh notation)] ä»£è¡¨å¢é•¿é‡çº§çš„ç¬¦å·ï¼›ä¾‹å¦‚ï¼Œ $O(n)$ ä»£è¡¨çº¿æ€§å¢é•¿çš„å‡½æ•°é›†åˆã€‚
\index{Big-Oh notation}

%ğŸ% \item[linear:] An algorithm whose run time is proportional to
%ğŸ% problem size, at least for large problem sizes.

\item[çº¿æ€§çº§ (linear)] ç®—æ³•çš„è¿è¡Œæ—¶é—´å’Œæ‰€æ±‚è§£é—®é¢˜çš„è§„æ¨¡æˆæ­£æ¯”ï¼Œ è‡³å°‘æ˜¯åœ¨é—®é¢˜è§„æ¨¡è¾ƒå¤§æ—¶æ˜¾ç°ã€‚
\index{linear}

%ğŸ% \item[quadratic:] An algorithm whose run time is proportional to
%ğŸ% $n^2$, where $n$ is a measure of problem size.

\item[äºŒæ¬¡æ–¹çº§ (quadratic)] ç®—æ³•çš„è¿è¡Œæ—¶é—´ å’Œ æ±‚è§£é—®é¢˜çš„è§„æ¨¡çš„äºŒæ¬¡æ–¹($n^2$)æˆæ­£æ¯”ï¼Œ$n$ ç”¨äºæè¿°é—®é¢˜çš„è§„æ¨¡ã€‚
\index{quadratic}

%ğŸ% \item[search:] The problem of locating an element of a collection
%ğŸ% (like a list or dictionary) or determining that it is not present.

\item[æœç´¢ (search)] åœ¨ä¸€ä¸ªé›†åˆï¼ˆä¾‹å¦‚åˆ—è¡¨æˆ–å­—å…¸ï¼‰ä¸­å®šä½æŸä¸ªå…ƒç´ ä½ç½®çš„é—®é¢˜ï¼Œä¹Ÿç­‰ä»·äºåˆ¤æ–­è¯¥å…ƒç´ æ˜¯å¦å­˜åœ¨äºé›†åˆä¸­ã€‚
\index{search}

%ğŸ% \item[hashtable:] A data structure that represents a collection of
%ğŸ% key-value pairs and performs search in constant time.

\item[å“ˆå¸Œè¡¨ (hashtable)] ä»£è¡¨é”®-å€¼å¯¹é›†åˆçš„ä¸€ç§æ•°æ®ç»“æ„ï¼Œæ‰§è¡Œæœç´¢æ“ä½œåªéœ€å¸¸æ•°æ—¶é—´ã€‚
\index{hashtable}

\end{description}
